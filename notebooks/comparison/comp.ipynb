{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Final Model Comparison for Thesis\n",
    "This notebook generates all comparison tables and figures needed for the thesis.\n",
    "\n",
    "**Run this AFTER running both DistilGPT2 and Qwen notebooks.**\n",
    "\n",
    "**Generates:**\n",
    "- Table 2: Model Comparison\n",
    "- Figure 4: Model Performance Bar Chart\n",
    "- Figure 5: Per-CWE Comparison\n",
    "- Figure 7: Efficiency vs Performance Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all model results\n",
    "\n",
    "def load_results(path, model_name):\n",
    "    \"\"\"Load results from JSON file\"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    if 'model_name' not in results:\n",
    "        results['model_name'] = model_name\n",
    "    return results\n",
    "\n",
    "# Load model results\n",
    "all_results = []\n",
    "model_files = [\n",
    "    ('results/eval_results_distilgpt.json', 'DistilGPT2'),\n",
    "    ('results/eval_results_qwen15.json', 'Qwen2.5-Coder-1.5B'),\n",
    "]\n",
    "\n",
    "for file_path, model_name in model_files:\n",
    "    try:\n",
    "        results = load_results(file_path, model_name)\n",
    "        all_results.append(results)\n",
    "        print(f\"âœ“ Loaded {model_name}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{model_name} results not found at {file_path}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(all_results)} models for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "table2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      "TABLE 2: MODEL COMPARISON - OVERALL PERFORMANCE\n",
      "========================================================================================================================\n",
      "                  Model Parameters  Trainable  Accuracy  Precision   Recall  F1-Score  Inference (ms) Training (s)\n",
      "             distilgpt2        N/A      16916  0.880702   0.888545 0.889516  0.887538        0.581030          N/A\n",
      "Qwen/Qwen2.5-Coder-1.5B        N/A    2394644  0.861404   0.858158 0.865028  0.856458        8.093195          N/A\n",
      "========================================================================================================================\n",
      "\n",
      "Table 2 saved as CSV and LaTeX\n"
     ]
    }
   ],
   "source": [
    "# Model Comparison\n",
    "\n",
    "comparison_data = []\n",
    "for results in all_results:\n",
    "    comparison_data.append({\n",
    "        'Model': results['model_name'],\n",
    "        'Parameters': results.get('total_parameters', 'N/A'),\n",
    "        # 'Trainable': results.get('trainable_parameters', 'N/A'),\n",
    "        'Trainable': results['trainable_params'],\n",
    "        'Accuracy': results['metrics']['test']['accuracy'],\n",
    "        'Precision': results['metrics']['test']['precision_macro'],\n",
    "        'Recall': results['metrics']['test']['recall_macro'],\n",
    "        'F1-Score': results['metrics']['test']['f1_macro'],\n",
    "        'Inference (ms)': results['metrics']['test']['inference_time_ms'],\n",
    "        'Training (s)': results.get('training_time_s', 'N/A')\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "df_comparison = df_comparison.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "\n",
    "print(\"TABLE 2: MODEL COMPARISON - OVERALL PERFORMANCE\")\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "# Save as CSV\n",
    "df_comparison.to_csv('results/table2_model_comparison.csv', index=False)\n",
    "# Save as LaTeX\n",
    "# latex_table = df_comparison.to_latex(index=False, float_format=\"%.4f\")\n",
    "# with open('results/table2_model_comparison.tex', 'w') as f:\n",
    "#     f.write(latex_table)\n",
    "\n",
    "# print(\"\\nTable 2 saved as CSV and LaTeX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figure4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Figure 4: Model Performance Comparison\n",
    "\n",
    "models = [r['model_name'] for r in all_results]\n",
    "metrics = {\n",
    "    'Accuracy': [r['metrics']['test']['accuracy'] for r in all_results],\n",
    "    'Precision': [r['metrics']['test']['precision_macro'] for r in all_results],\n",
    "    'Recall': [r['metrics']['test']['recall_macro'] for r in all_results],\n",
    "    'F1-Score': [r['metrics']['test']['f1_macro'] for r in all_results]\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(models))\n",
    "width = 0.2\n",
    "\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "for i, (metric, values) in enumerate(metrics.items()):\n",
    "    offset = width * (i - 1.5)\n",
    "    bars = ax.bar(x + offset, values, width, label=metric, color=colors[i], alpha=0.8)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "ax.set_xlabel('Model', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Score', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Model Performance Comparison Across Metrics', fontweight='bold', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/figure4_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figures/figure4_model_comparison.pdf', bbox_inches='tight')\n",
    "print(\"Figure 4 saved: figures/figure4_model_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "# just F1 scores\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "f1_scores = [r['metrics']['test']['f1_macro'] for r in all_results]\n",
    "bars = ax.bar(models, f1_scores, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'][:len(models)], alpha=0.8)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.4f}',\n",
    "            ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "ax.set_xlabel('Model', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('F1-Score (Macro)', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Model F1-Score Comparison', fontweight='bold', fontsize=14)\n",
    "ax.set_ylim(0, max(f1_scores) * 1.15)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/figure4b_f1_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figures/figure4b_f1_comparison.pdf', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figure5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Per-CWE Performance Comparison\n",
    "\n",
    "# Collect per-CWE F1 scores from all models\n",
    "all_cwes = set()\n",
    "for results in all_results:\n",
    "    if 'per_class_test' in results:\n",
    "        all_cwes.update(results['per_class_test'].keys())\n",
    "\n",
    "print(all_cwes)\n",
    "\n",
    "cwe_comparison = {}\n",
    "for cwe in sorted(all_cwes, key=lambda x: int(x) if str(x).isdigit() else 0):\n",
    "    cwe_comparison[cwe] = {}\n",
    "    for results in all_results:\n",
    "        if 'per_class_test' in results and cwe in results['per_class_test']:\n",
    "            model_name = results['model_name']\n",
    "            cwe_comparison[cwe][model_name] = results['per_class_test'][cwe]['f1']\n",
    "\n",
    "df_cwe = pd.DataFrame(cwe_comparison).T\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "df_cwe.plot(kind='bar', ax=ax, width=0.8, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'][:len(all_results)])\n",
    "\n",
    "ax.set_xlabel('CWE Type', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('F1-Score', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Per-CWE Performance Across Models', fontweight='bold', fontsize=14)\n",
    "ax.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figures/figure5_per_cwe_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figures/figure5_per_cwe_comparison.pdf', bbox_inches='tight')\n",
    "print(\"Figure 5 saved: figures/figure5_per_cwe_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "df_cwe.to_csv('results/table3_per_cwe_comparison_all_models.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figure7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiency vs Performance Trade-off\n",
    "\n",
    "# Filter models with inference time data\n",
    "models_with_inference = [r for r in all_results if r['metrics']['test']['inference_time_ms'] > 0]\n",
    "\n",
    "if len(models_with_inference) > 0:\n",
    "    model_names = [r['model_name'] for r in models_with_inference]\n",
    "    f1_scores = [r['metrics']['test']['f1_macro'] for r in models_with_inference]\n",
    "    inference_times = [r['metrics']['test']['inference_time_ms'] for r in models_with_inference]\n",
    "    params = [r.get('total_parameters', 0) for r in models_with_inference]\n",
    "    \n",
    "    # Normalize parameter count for marker size\n",
    "    max_params = max(params) if max(params) > 0 else 1\n",
    "    marker_sizes = [200 + (p / max_params) * 800 for p in params]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c']\n",
    "    for i, (name, f1, inf_time, size) in enumerate(zip(model_names, f1_scores, inference_times, marker_sizes)):\n",
    "        ax.scatter(inf_time, f1, s=size, alpha=0.6, c=[colors[i % len(colors)]], edgecolors='black', linewidth=1.5)\n",
    "        ax.annotate(name, (inf_time, f1), xytext=(10, 5), textcoords='offset points',\n",
    "                   fontsize=10, fontweight='bold', \n",
    "                   bbox=dict(boxstyle='round,pad=0.3', facecolor=colors[i % len(colors)], alpha=0.3))\n",
    "    \n",
    "    ax.set_xlabel('Inference Time (ms/sample)', fontweight='bold', fontsize=12)\n",
    "    ax.set_ylabel('F1-Score (Macro)', fontweight='bold', fontsize=12)\n",
    "    ax.set_title('Model Efficiency vs Performance Trade-off', fontweight='bold', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add ideal region shading (top-left = fast + accurate)\n",
    "    ax.axhspan(0.7, 1.0, alpha=0.05, color='green', label='High Performance')\n",
    "    if max(inference_times) > 10:\n",
    "        ax.axvspan(0, np.percentile(inference_times, 50), alpha=0.05, color='blue', label='Fast Inference')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/figure7_efficiency_tradeoff.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig('figures/figure7_efficiency_tradeoff.pdf', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figure8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Size vs Performance\n",
    "\n",
    "models_with_params = [r for r in all_results if r.get('total_parameters', 0) > 0]\n",
    "\n",
    "if len(models_with_params) > 0:\n",
    "    model_names = [r['model_name'] for r in models_with_params]\n",
    "    f1_scores = [r['f1_macro'] for r in models_with_params]\n",
    "    params = [r['total_parameters'] / 1e6 for r in models_with_params]  # Convert to millions\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c']\n",
    "    bars = ax.bar(range(len(model_names)), f1_scores, color=[colors[i % len(colors)] for i in range(len(model_names))], alpha=0.7)\n",
    "    \n",
    "    # Add parameter count as text on bars\n",
    "    for i, (bar, param) in enumerate(zip(bars, params)):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}\\n({param:.1f}M params)',\n",
    "                ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('Model', fontweight='bold', fontsize=12)\n",
    "    ax.set_ylabel('F1-Score (Macro)', fontweight='bold', fontsize=12)\n",
    "    ax.set_title('Model Size vs Performance', fontweight='bold', fontsize=14)\n",
    "    ax.set_xticks(range(len(model_names)))\n",
    "    ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    ax.set_ylim(0, max(f1_scores) * 1.2)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/figure8_model_size_vs_performance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig('figures/figure8_model_size_vs_performance.pdf', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STATISTICAL SUMMARY: PER-CWE F1-SCORE DISTRIBUTION\n",
      "================================================================================\n",
      "                  Model  Mean F1   Std F1   Min F1  Max F1  Median F1\n",
      "             distilgpt2 0.887538 0.237348 0.234043     1.0   0.984495\n",
      "Qwen/Qwen2.5-Coder-1.5B 0.856458 0.263951 0.000000     1.0   0.973786\n",
      "================================================================================\n",
      "\n",
      "Statistical summary saved to results/statistical_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Statistical Summary Table\n",
    "\n",
    "# Create detailed statistical summary\n",
    "summary_stats = []\n",
    "for results in all_results:\n",
    "    per_class = results.get('per_class_test', {})\n",
    "    if per_class:\n",
    "        f1_values = [m['f1'] for m in per_class.values()]\n",
    "        summary_stats.append({\n",
    "            'Model': results['model_name'],\n",
    "            'Mean F1': np.mean(f1_values),\n",
    "            'Std F1': np.std(f1_values),\n",
    "            'Min F1': np.min(f1_values),\n",
    "            'Max F1': np.max(f1_values),\n",
    "            'Median F1': np.median(f1_values)\n",
    "        })\n",
    "\n",
    "if summary_stats:\n",
    "    df_stats = pd.DataFrame(summary_stats)\n",
    "    \n",
    "    print(\"STATISTICAL SUMMARY: PER-CWE F1-SCORE DISTRIBUTION\")\n",
    "    print(df_stats.to_string(index=False))\n",
    "    \n",
    "    df_stats.to_csv('results/statistical_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick_stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Statistics for Writing\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    print(\"QUICK STATS FOR YOUR THESIS WRITING\")\n",
    "    \n",
    "    # Best model\n",
    "    best = max(all_results, key=lambda x: x['f1_macro'])\n",
    "    print(f\"\\n Best Model: {best['model_name']}\")\n",
    "    print(f\"  - F1-Score: {best['f1_macro']:.4f}\")\n",
    "    print(f\"  - Accuracy: {best['accuracy']:.4f}\")\n",
    "    print(f\"  - Precision: {best['precision_macro']:.4f}\")\n",
    "    print(f\"  - Recall: {best['recall_macro']:.4f}\")\n",
    "\n",
    "    # Comparison with worst\n",
    "    worst = min(all_results, key=lambda x: x['f1_macro'])\n",
    "    improvement = ((best['f1_macro'] - worst['f1_macro']) / worst['f1_macro'] * 100)\n",
    "    print(f\"\\nPerformance Range:\")\n",
    "    print(f\"   - Best: {best['model_name']} (F1={best['f1_macro']:.4f})\")\n",
    "    print(f\"   - Worst: {worst['model_name']} (F1={worst['f1_macro']:.4f})\")\n",
    "    print(f\"   - Relative Improvement: {improvement:.1f}%\")\n",
    "    \n",
    "    # Efficiency analysis\n",
    "    with_inference = [r for r in all_results if r.get('inference_time_ms', 0) > 0]\n",
    "    if with_inference:\n",
    "        fastest = min(with_inference, key=lambda x: x['inference_time_ms'])\n",
    "        print(f\"\\nMost Efficient:\")\n",
    "        print(f\"   - Model: {fastest['model_name']}\")\n",
    "        print(f\"   - Inference Time: {fastest['inference_time_ms']:.2f} ms/sample\")\n",
    "        print(f\"   - F1-Score: {fastest['f1_macro']:.4f}\")\n",
    "        print(f\"   - Throughput: ~{1000/fastest['inference_time_ms']:.0f} samples/second\")\n",
    "    \n",
    "    # Sentences for thesis\n",
    "    print(\"\\nSample Sentences for Your Thesis:\")\n",
    "    print(\"\\n1. Abstract/Introduction:\")\n",
    "    print(f'   \"We evaluate {len(all_results)} models for CWE classification in Go code, ')\n",
    "    print(f'   achieving a best F1-score of {best[\"f1_macro\"]:.4f} with {best[\"model_name\"]}.\"')\n",
    "    \n",
    "    print(\"\\n2. Results:\")\n",
    "    print(f'   \"Our experiments show that {best[\"model_name\"]} outperforms baseline methods ')\n",
    "    print(f'   by {improvement:.1f}%, demonstrating the effectiveness of small language models.\"')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary Report\n",
    "\n",
    "print(\"\\nBest Performing Models:\")\n",
    "sorted_results = sorted(all_results, key=lambda x: x['f1_macro'], reverse=True)\n",
    "for i, r in enumerate(sorted_results[:3], 1):\n",
    "    print(f\"  {i}. {r['model_name']}: F1={r['f1_macro']:.4f}, Acc={r['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nMost Efficient Model (if data available):\")\n",
    "efficient = [r for r in all_results if r.get('inference_time_ms', 0) > 0]\n",
    "if efficient:\n",
    "    fastest = min(efficient, key=lambda x: x['inference_time_ms'])\n",
    "    print(f\"  {fastest['model_name']}: {fastest['inference_time_ms']:.2f} ms/sample, F1={fastest['f1_macro']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slm-go",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
