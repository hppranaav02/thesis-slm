{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c98b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5793d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Set CUDA device explicitly first\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3739188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure quantization for CodeGen-350M\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Model directory - CodeGen-350M\n",
    "model_dir = \"Salesforce/codegen-350M-multi\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "\n",
    "# Add pad token if it doesn't exist (common issue with CodeGen)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=bnb_config,   \n",
    "    device_map={\"\": torch.cuda.current_device()},\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True             \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "print(f\"Model loaded: {model_dir}\")\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f152ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training prompt style for cybersecurity CWE classification\n",
    "# Option 1: Instruction-style (your current approach)\n",
    "train_prompt_style_instruction = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a cybersecurity expert with advanced knowledge in software security and common weakness enumerations(CWE). \n",
    "Look at the following code and classify it with the apropriate CWE's if it has any. \n",
    "\n",
    "### Code:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "# Option 2: Code-comment style (better for CodeGen)\n",
    "train_prompt_style_comment = \"\"\"/*\n",
    "Security Analysis Task:\n",
    "Analyze the following code for Common Weakness Enumerations (CWE).\n",
    "Identify any security vulnerabilities and classify them with appropriate CWE numbers.\n",
    "\n",
    "Code to analyze:\n",
    "*/\n",
    "{}\n",
    "\n",
    "/*\n",
    "Security Analysis Result:\n",
    "{}\n",
    "*/\"\"\"\n",
    "\n",
    "# Option 3: Simple completion style\n",
    "train_prompt_style_simple = \"\"\"// Security vulnerability analysis\n",
    "// Code:\n",
    "{}\n",
    "\n",
    "// CWE Classification:\n",
    "{}\"\"\"\n",
    "\n",
    "# Option 4: Q&A style (simpler than instruction)\n",
    "train_prompt_style_qa = \"\"\"Question: What CWE vulnerabilities are present in this code?\n",
    "\n",
    "Code:\n",
    "{}\n",
    "\n",
    "Answer: {}\"\"\"\n",
    "\n",
    "# Choose which style to use (default to comment style for CodeGen)\n",
    "train_prompt_style = train_prompt_style_comment\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for code, response in zip(inputs, outputs):\n",
    "        # Append the EOS token to the response if it's not already there\n",
    "        if not response.endswith(tokenizer.eos_token):\n",
    "            response += tokenizer.eos_token\n",
    "        text = train_prompt_style.format(code, response)\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1123b214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load your dataset (adjust path as needed)\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_dir=\"/local/s3905020/code/dataset-creation\"\n",
    ")[\"train\"]\n",
    "\n",
    "# Format the dataset output field (convert list to string if needed)\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"output\": [\" \".join(map(str, out)) if isinstance(out, list) else str(out) for out in x[\"output\"]]}, \n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Apply formatting function\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Inspect some examples\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "print(\"\\nExample 1:\")\n",
    "print(dataset[\"text\"][10][:500] + \"...\" if len(dataset[\"text\"][10]) > 500 else dataset[\"text\"][10])\n",
    "\n",
    "print(\"\\nExample 2:\")\n",
    "print(dataset[\"text\"][100][:500] + \"...\" if len(dataset[\"text\"][100]) > 500 else dataset[\"text\"][100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe7636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d321cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match the training prompt style\n",
    "inference_prompt_style_instruction = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a cybersecurity expert with advanced knowledge in software security and common weakness enumerations (CWE). \n",
    "Please look at the following code and classify it with the appropriate CWE's if it has any.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "\"\"\"\n",
    "\n",
    "inference_prompt_style_comment = \"\"\"/*\n",
    "Security Analysis Task:\n",
    "Analyze the following code for Common Weakness Enumerations (CWE).\n",
    "Identify any security vulnerabilities and classify them with appropriate CWE numbers.\n",
    "\n",
    "Code to analyze:\n",
    "*/\n",
    "{}\n",
    "\n",
    "/*\n",
    "Security Analysis Result:\n",
    "\"\"\"\n",
    "\n",
    "inference_prompt_style_simple = \"\"\"// Security vulnerability analysis\n",
    "// Code:\n",
    "{}\n",
    "\n",
    "// CWE Classification:\n",
    "\"\"\"\n",
    "\n",
    "inference_prompt_style_qa = \"\"\"Question: What CWE vulnerabilities are present in this code?\n",
    "\n",
    "Code:\n",
    "{}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "# Match the training style\n",
    "inference_prompt_style = inference_prompt_style_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a8456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Use the device that the model was loaded on\n",
    "device = model.device\n",
    "print(f\"Model is on device: {device}\")\n",
    "\n",
    "# Test base model before fine-tuning\n",
    "question = dataset[10]['input']\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt_style.format(question)],\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=2048\n",
    ").to(device)\n",
    "\n",
    "print(\"Testing base model...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_new_tokens=1200,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(\"Base model response:\")\n",
    "print(response[0].split(\"### Response:\")[1] if \"### Response:\" in response[0] else response[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a110c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# LoRA config adapted for CodeGen-350M\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,                           # Scaling factor for LoRA\n",
    "    lora_dropout=0.05,                       # Add slight dropout for regularization\n",
    "    r=64,                                    # Rank of the LoRA update matrices\n",
    "    bias=\"none\",                             # No bias reparameterization\n",
    "    task_type=\"CAUSAL_LM\",                   # Task type: Causal Language Modeling\n",
    "    target_modules=[\n",
    "        \"qkv_proj\",                          # CodeGen uses qkv_proj instead of separate q,k,v\n",
    "        \"out_proj\",                          # Output projection\n",
    "        \"fc_in\",                             # Feed-forward input\n",
    "        \"fc_out\",                            # Feed-forward output\n",
    "    ],  # Target modules for LoRA (adapted for CodeGen architecture)\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dbbf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training Arguments (adjusted for CodeGen-350M)\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"codegen_output\",\n",
    "    per_device_train_batch_size=2,           # Increased batch size for smaller model\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=3,                      # Reduced epochs for faster training\n",
    "    logging_steps=0.1,                       # More frequent logging\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=3e-4,                      # Slightly higher LR for smaller model\n",
    "    fp16=False,\n",
    "    bf16=True,                               # Use bf16 if available\n",
    "    group_by_length=True,\n",
    "    report_to=\"wandb\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"no\",\n",
    "    max_grad_norm=1.0,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Cell 11: Initialize Trainer\n",
    "# Initialize the SFT Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    data_collator=data_collator,\n",
    "    dataset_text_field=\"text\",               # Specify the text field\n",
    "    max_seq_length=2048,                     # Set max sequence length\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812ab264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory before training\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Ensure the trainer uses the same device as the model\n",
    "device = model.device\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "# Check if the model is properly loaded on the expected device\n",
    "if hasattr(model, 'base_model'):\n",
    "    print(f\"Base model device: {model.base_model.device}\")\n",
    "elif hasattr(model, 'model'):\n",
    "    print(f\"Model's model device: {model.model.device}\")\n",
    "\n",
    "# Make sure all model parts are on the same device\n",
    "model = model.to(device)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce1e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78334cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"codegen_final_model\")\n",
    "tokenizer.save_pretrained(\"codegen_final_model\")\n",
    "print(\"Model saved to codegen_final_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ab2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing fine-tuned model...\")\n",
    "\n",
    "# Test on a different sample\n",
    "question = dataset[100]['input']\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt_style.format(question)],\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=2048\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_new_tokens=1200,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(\"Fine-tuned model response:\")\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799bbdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer_codegen(model_dir=\"Salesforce/codegen-350M-multi\", \n",
    "                                     checkpoint_path=\"./codegen_output/checkpoint-XXX\",\n",
    "                                     device_str=\"cuda:0\"):\n",
    "    \"\"\"Load fine-tuned CodeGen model for evaluation\"\"\"\n",
    "    print(f\"Loading model from {model_dir} and checkpoint from {checkpoint_path}\")\n",
    "    \n",
    "    # Configure quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    # Set device\n",
    "    if device_str.startswith(\"cuda\"):\n",
    "        device_id = int(device_str.split(\":\")[-1])\n",
    "        torch.cuda.set_device(device_id)\n",
    "        print(f\"Using CUDA device: {torch.cuda.get_device_name(device_id)}\")\n",
    "    \n",
    "    device = torch.device(device_str)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load base model with quantization\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": device.index if device.type == \"cuda\" else \"cpu\"},\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Load the PEFT adapter\n",
    "    from peft import PeftModel\n",
    "    model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "    model.config.use_cache = True\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6cfa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset_by_tokens_codegen(file_path, max_tokens=100000, output_file=None):\n",
    "    \"\"\"Filter dataset for CodeGen tokenizer\"\"\"\n",
    "    import json\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-multi\", use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    filtered_data = []\n",
    "    excluded_count = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    print(f\"Filtering dataset: {file_path}\")\n",
    "    print(f\"Maximum tokens allowed: {max_tokens}\")\n",
    "    \n",
    "    # Process each line in the JSONL file\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            total_count += 1\n",
    "            entry = json.loads(line)\n",
    "            \n",
    "            # Tokenize the input text\n",
    "            tokens = tokenizer(entry[\"input\"], return_length=True, truncation=False)\n",
    "            token_count = len(tokens[\"input_ids\"])\n",
    "            \n",
    "            # Include entry if it's below the token limit\n",
    "            if token_count < max_tokens:\n",
    "                filtered_data.append(entry)\n",
    "            else:\n",
    "                excluded_count += 1\n",
    "    \n",
    "    # Save filtered dataset if output file is specified\n",
    "    if output_file:\n",
    "        with open(output_file, 'w') as f:\n",
    "            for entry in filtered_data:\n",
    "                f.write(json.dumps(entry) + '\\n')\n",
    "        print(f\"Filtered dataset saved to: {output_file}\")\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Total entries: {total_count}\")\n",
    "    print(f\"Entries kept: {len(filtered_data)} ({len(filtered_data)/total_count*100:.2f}%)\")\n",
    "    print(f\"Entries excluded: {excluded_count} ({excluded_count/total_count*100:.2f}%)\")\n",
    "    \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f45e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_codegen_model(model, tokenizer, test_data, device, output_file=\"codegen_evaluation_results.jsonl\"):\n",
    "    \"\"\"Evaluate CodeGen model on test data\"\"\"\n",
    "    import json\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    print(f\"Evaluating CodeGen model on {len(test_data)} test samples\")\n",
    "    results = []\n",
    "    \n",
    "    for idx, item in enumerate(tqdm(test_data)):\n",
    "        code = item[\"input\"]\n",
    "        expected_output = item[\"output\"]\n",
    "        \n",
    "        prompt = inference_prompt_style.format(code)\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            [prompt], \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            max_length=2048\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_new_tokens=1200,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                use_cache=True,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        response = generated_text.split(\"### Response:\")[1].strip() if \"### Response:\" in generated_text else generated_text\n",
    "        \n",
    "        result = {\n",
    "            \"index\": idx,\n",
    "            \"input\": code,\n",
    "            \"expected_output\": expected_output,\n",
    "            \"model_output\": response,\n",
    "            \"full_response\": generated_text\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # Save results periodically\n",
    "        if idx % 10 == 0:\n",
    "            with open(output_file, 'w') as f:\n",
    "                for res in results:\n",
    "                    f.write(json.dumps(res) + '\\n')\n",
    "    \n",
    "    # Final save\n",
    "    with open(output_file, 'w') as f:\n",
    "        for res in results:\n",
    "            f.write(json.dumps(res) + '\\n')\n",
    "    \n",
    "    print(f\"Evaluation complete! Results saved to {output_file}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee6f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_memory():\n",
    "    \"\"\"Clean up GPU memory\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b184ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "evaluation:\n",
    "\n",
    "# 1. Filter test dataset\n",
    "test_file = \"/path/to/test.jsonl\"\n",
    "filtered_file = \"/path/to/test_filtered.jsonl\"\n",
    "filtered_data = filter_dataset_by_tokens_codegen(test_file, max_tokens=80000, output_file=filtered_file)\n",
    "\n",
    "# 2. Load fine-tuned model\n",
    "model, tokenizer, device = load_model_and_tokenizer_codegen(\n",
    "    checkpoint_path=\"./codegen_output/checkpoint-1000\"\n",
    ")\n",
    "\n",
    "# 3. Load test data\n",
    "import json\n",
    "test_data = []\n",
    "with open(filtered_file, 'r') as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))\n",
    "\n",
    "# 4. Run evaluation\n",
    "results = evaluate_codegen_model(model, tokenizer, test_data, device)\n",
    "\n",
    "# 5. Calculate metrics (use the same metrics functions from the original notebook)\n",
    "# You can copy the calculate_metrics and visualization functions from the original notebook\n",
    "\"\"\"\n",
    "\n",
    "print(\"CodeGen-350M fine-tuning notebook ready!\")\n",
    "print(\"Adjust the dataset paths and run the cells sequentially.\")\n",
    "print(\"The model architecture differences from Qwen have been handled in the LoRA configuration.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
