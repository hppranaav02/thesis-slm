{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c98b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5793d20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/s3905020/slm-go/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Set CUDA device explicitly first\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3739188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Salesforce/codegen-350M-multi were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Salesforce/codegen-350M-multi\n",
      "Tokenizer vocab size: 50257\n"
     ]
    }
   ],
   "source": [
    "# Configure quantization for CodeGen-350M\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Model directory - CodeGen-350M\n",
    "model_dir = \"Salesforce/codegen-350M-multi\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "\n",
    "# Add pad token if it doesn't exist (common issue with CodeGen)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=bnb_config,   \n",
    "    device_map={\"\": torch.cuda.current_device()},\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True             \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "print(f\"Model loaded: {model_dir}\")\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f152ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training prompt style for cybersecurity CWE classification\n",
    "# Option 1: Instruction-style (your current approach)\n",
    "train_prompt_style_instruction = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a cybersecurity expert with advanced knowledge in software security and common weakness enumerations(CWE). \n",
    "Look at the following code and classify it with the apropriate CWE's if it has any. \n",
    "\n",
    "### Code:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "# Option 2: Code-comment style (better for CodeGen)\n",
    "train_prompt_style_comment = \"\"\"/*\n",
    "Security Analysis Task:\n",
    "Analyze the following code for Common Weakness Enumerations (CWE).\n",
    "Identify any security vulnerabilities and classify them with appropriate CWE numbers.\n",
    "\n",
    "Code to analyze:\n",
    "*/\n",
    "{}\n",
    "\n",
    "/*\n",
    "Security Analysis Result:\n",
    "{}\n",
    "*/\"\"\"\n",
    "\n",
    "# Option 3: Simple completion style\n",
    "train_prompt_style_simple = \"\"\"// Security vulnerability analysis\n",
    "// Code:\n",
    "{}\n",
    "\n",
    "// CWE Classification:\n",
    "{}\"\"\"\n",
    "\n",
    "# Option 4: Q&A style (simpler than instruction)\n",
    "train_prompt_style_qa = \"\"\"Question: What CWE vulnerabilities are present in this code?\n",
    "\n",
    "Code:\n",
    "{}\n",
    "\n",
    "Answer: {}\"\"\"\n",
    "\n",
    "# Choose which style to use (default to comment style for CodeGen)\n",
    "train_prompt_style = train_prompt_style_comment\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    # Create comma seperated string from the list of outputs\n",
    "    outputs = [\", \".join(output) if isinstance(output, list) else output for output in outputs]\n",
    "    for code, response in zip(inputs, outputs):\n",
    "        # Append the EOS token to the response if it's not already there\n",
    "        if not response.endswith(tokenizer.eos_token):\n",
    "            response += tokenizer.eos_token\n",
    "        text = train_prompt_style.format(code, response)\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1123b214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4656/4656 [00:00<00:00, 11780.28 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 4656\n",
      "\n",
      "Example 1:\n",
      "/*\n",
      "Security Analysis Task:\n",
      "Analyze the following code for Common Weakness Enumerations (CWE).\n",
      "Identify any security vulnerabilities and classify them with appropriate CWE numbers.\n",
      "\n",
      "Code to analyze:\n",
      "*/\n",
      "package models\n",
      "\n",
      "import (\n",
      "\t\"crypto/tls\"\n",
      "\t\"errors\"\n",
      "\t\"net/mail\"\n",
      "\t\"os\"\n",
      "\t\"strconv\"\n",
      "\t\"strings\"\n",
      "\t\"time\"\n",
      "\n",
      "\t\"github.com/gophish/gomail\"\n",
      "\tlog \"github.com/gophish/gophish/logger\"\n",
      "\t\"github.com/gophish/gophish/mailer\"\n",
      "\t\"github.com/jinzhu/gorm\"\n",
      ")\n",
      "\n",
      "// Dialer is a wrapper around a standard gomail.Dialer in order\n",
      "/...\n",
      "\n",
      "Example 2:\n",
      "/*\n",
      "Security Analysis Task:\n",
      "Analyze the following code for Common Weakness Enumerations (CWE).\n",
      "Identify any security vulnerabilities and classify them with appropriate CWE numbers.\n",
      "\n",
      "Code to analyze:\n",
      "*/\n",
      "package utils\n",
      "\n",
      "import (\n",
      "\t\"net/url\"\n",
      "\tstdpath \"path\"\n",
      "\t\"path/filepath\"\n",
      "\t\"runtime\"\n",
      "\t\"strings\"\n",
      ")\n",
      "\n",
      "// StandardizePath convert path like '/' '/root' '/a/b'\n",
      "func StandardizePath(path string) string {\n",
      "\tpath = strings.TrimSuffix(path, \"/\")\n",
      "\t// abs path\n",
      "\tif filepath.IsAbs(path) && runtime.GOOS == \"windows\" {\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load your dataset (adjust path as needed)\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_dir=\"/local/s3905020/code/dataset-creation\"\n",
    ")[\"train\"]\n",
    "\n",
    "# Format the dataset output field (convert list to string if needed)\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"output\": [\" \".join(map(str, out)) if isinstance(out, list) else str(out) for out in x[\"output\"]]}, \n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Apply formatting function\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Inspect some examples\n",
    "print(\"Dataset size:\", len(dataset))\n",
    "print(\"\\nExample 1:\")\n",
    "print(dataset[\"text\"][10][:500] + \"...\" if len(dataset[\"text\"][10]) > 500 else dataset[\"text\"][10])\n",
    "\n",
    "print(\"\\nExample 2:\")\n",
    "print(dataset[\"text\"][100][:500] + \"...\" if len(dataset[\"text\"][100]) > 500 else dataset[\"text\"][100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fe7636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d321cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match the training prompt style\n",
    "inference_prompt_style_instruction = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a cybersecurity expert with advanced knowledge in software security and common weakness enumerations (CWE). \n",
    "Please look at the following code and classify it with the appropriate CWE's if it has any.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "\"\"\"\n",
    "\n",
    "inference_prompt_style_comment = \"\"\"/*\n",
    "Security Analysis Task:\n",
    "Analyze the following code for Common Weakness Enumerations (CWE).\n",
    "Identify any security vulnerabilities and classify them with appropriate CWE numbers.\n",
    "\n",
    "Code to analyze:\n",
    "*/\n",
    "{}\n",
    "\n",
    "/*\n",
    "Security Analysis Result:\n",
    "\"\"\"\n",
    "\n",
    "inference_prompt_style_simple = \"\"\"// Security vulnerability analysis\n",
    "// Code:\n",
    "{}\n",
    "\n",
    "// CWE Classification:\n",
    "\"\"\"\n",
    "\n",
    "inference_prompt_style_qa = \"\"\"Question: What CWE vulnerabilities are present in this code?\n",
    "\n",
    "Code:\n",
    "{}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "# Match the training style\n",
    "inference_prompt_style = inference_prompt_style_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86a8456d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cuda:0\n",
      "GPU Memory allocated: 345.42 MB\n",
      "GPU Memory reserved: 468.00 MB\n",
      "Max memory allocated: 445.32 MB\n",
      "Testing base model...\n",
      "Base model response:\n",
      "/*\n",
      "Security Analysis Task:\n",
      "Analyze the following code for Common Weakness Enumerations (CWE).\n",
      "Identify any security vulnerabilities and classify them with appropriate CWE numbers.\n",
      "\n",
      "Code to analyze:\n",
      "*/\n",
      "package models\n",
      "\n",
      "import (\n",
      "\t\"crypto/tls\"\n",
      "\t\"errors\"\n",
      "\t\"net/mail\"\n",
      "\t\"os\"\n",
      "\t\"strconv\"\n",
      "\t\"strings\"\n",
      "\t\"time\"\n",
      "\n",
      "\t\"github.com/gophish/gomail\"\n",
      "\tlog \"github.com/gophish/gophish/logger\"\n",
      "\t\"github.com/gophish/gophish/mailer\"\n",
      "\t\"github.com/jinzhu/gorm\"\n",
      ")\n",
      "\n",
      "// Dialer is a wrapper around a standard gomail.Dialer in order\n",
      "// to implement the mailer.Dialer interface. This allows us to better\n",
      "// separate the mailer package as opposed to forcing a connection\n",
      "// between mailer and gomail.\n",
      "type Dialer struct {\n",
      "\t*gomail.Dialer\n",
      "}\n",
      "\n",
      "// Dial wraps the gomail dialer's Dial command\n",
      "func (d *Dialer) Dial() (mailer.Sender, error) {\n",
      "\treturn d.Dialer.Dial()\n",
      "}\n",
      "\n",
      "// SMTP contains the attributes needed to handle the sending of campaign emails\n",
      "type SMTP struct {\n",
      "\tId               int64     `json:\"id\" gorm:\"column:id; primary_key:yes\"`\n",
      "\tUserId           int64     `json:\"-\" gorm:\"column:user_id\"`\n",
      "\tInterface        string    `json:\"interface_type\" gorm:\"column:interface_type\"`\n",
      "\tName             string    `json:\"name\"`\n",
      "\tHost             string    `json:\"host\"`\n",
      "\tUsername         string    `json:\"username,omitempty\"`\n",
      "\tPassword         string    `json:\"password,omitempty\"`\n",
      "\tFromAddress      string    `json:\"from_address\"`\n",
      "\tIgnoreCertErrors bool      `json:\"ignore_cert_errors\"`\n",
      "\tHeaders          []Header  `json:\"headers\"`\n",
      "\tModifiedDate     time.Time `json:\"modified_date\"`\n",
      "}\n",
      "\n",
      "// Header contains the fields and methods for a sending campaign\n",
      "type Header struct {\n",
      "\tFields []string `json:\"fields\"`\n",
      "\tMethods []string `json:\"methods\"`\n",
      "}\n",
      "\n",
      "// AddHeader adds a new header to the message\n",
      "func (m *SMTP) AddHeader(header string) error {\n",
      "\tif m.IgnoreCertErrors {\n",
      "\t\treturn errors.New(\"IgnoreCertErrors can only be used with non-root emails\")\n",
      "\t}\n",
      "\tif len(m.Fields) == 0 {\n",
      "\t\treturn errors.New(\"Fields must be set\")\n",
      "\t}\n",
      "\tif len(m.Methods) == 0 {\n",
      "\t\treturn errors.New(\"Methods must be set\")\n",
      "\t}\n",
      "\tfor _, f := range m.Fields {\n",
      "\t\tif !strings.Contains(f, \":\") {\n",
      "\t\t\treturn errors.New(\"Fields must contain ':'\")\n",
      "\t\t}\n",
      "\t}\n",
      "\tfor _, m := range m.Methods {\n",
      "\t\tif !strings.Contains(m, \":\") {\n",
      "\t\t\treturn errors.New(\"Methods must contain ':'\")\n",
      "\t\t}\n",
      "\t}\n",
      "\tif len(header) == 0 {\n",
      "\t\treturn errors.New(\"header cannot be empty\")\n",
      "\t}\n",
      "\treturn nil\n",
      "}\n",
      "\n",
      "// Send sends a message to the specified recipient and subject\n",
      "func (m *SMTP) Send(recipient string, subject string, data []byte) error {\n",
      "\tif m.IgnoreCertErrors {\n",
      "\t\treturn errors.New(\"IgnoreCertErrors can only be used with non-root emails\")\n",
      "\t}\n",
      "\tif len(m.Fields) == 0 {\n",
      "\t\treturn errors.New(\"Fields must be set\")\n",
      "\t}\n",
      "\tif len(m.Methods) == 0 {\n",
      "\t\treturn errors.New(\"Methods must be set\")\n",
      "\t}\n",
      "\tfor _, f := range m.Fields {\n",
      "\t\tif !strings.Contains(f, \":\") {\n",
      "\t\t\treturn errors.New(\"Fields must contain ':'\")\n",
      "\t\t}\n",
      "\t}\n",
      "\tfor _, m := range m.Methods {\n",
      "\t\tif !strings.Contains(m, \":\") {\n",
      "\t\t\treturn errors.New(\"Methods must contain ':'\")\n",
      "\t\t}\n",
      "\t}\n",
      "\tif len(m.Fields) == 0 {\n",
      "\t\treturn errors.New(\"Fields must be set\")\n",
      "\t}\n",
      "\tif len(m.Methods) == 0 {\n",
      "\t\treturn errors.New(\"Methods must be set\")\n",
      "\t}\n",
      "\tif len(recipient) == 0 {\n",
      "\t\treturn errors.New(\"recipient cannot be empty\")\n",
      "\t}\n",
      "\tif len(subject) == 0 {\n",
      "\t\treturn errors.New(\"subject cannot be empty\")\n",
      "\t}\n",
      "\tm.ModifiedDate = time.Now()\n",
      "\tm.FromAddress = email.Address(recipient)\n",
      "\tm.Username = email.Address(subject)\n",
      "\tm.Password = email.Password(data)\n",
      "\tm.Headers = append(m.Headers, Header{\n",
      "\t\tFields: strings.Join(m.Fields, \",\"),\n",
      "\t\tMethods: strings.Join(m.Methods, \",\"),\n",
      "\t})\n",
      "\treturn nil\n",
      "}\n",
      "\n",
      "// Send sends a message to the specified recipient and subject\n",
      "func (m *SMTP) Send(recipient string, subject string, data []byte) error {\n",
      "\treturn m.Send(recipient, subject, data)\n",
      "}\n",
      "\n",
      "// Dialer is the standard gomail.Dialer for sending emails\n",
      "type Dialer struct {\n",
      "\t*gomail.Dialer\n",
      "}\n",
      "\n",
      "// Dial dials the gomail.Dialer and returns a new gomail.SMTP\n",
      "func Dial(interfaceName string) (mailer.Sender, error) {\n",
      "\treturn Dialer{\n",
      "\t\tDialer: gomail.Dialer{\n",
      "\t\t\tHost:       \"\",\n",
      "\t\t\tUsername:   \"\",\n",
      "\t\t\tPassword:   \"\",\n",
      "\t\t\tFrom:       \"\",\n",
      "\t\t\tUserAgent:  \"\",\n",
      "\t\t\tTLSConfig:  &tls.Config{InsecureSkipVerify: true},\n",
      "\t\t\tTLSClientConfig: &tls.Config{InsecureSkipVerify: true},\n",
      "\t\t},\n",
      "\t}, nil\n",
      "}\n",
      "\n",
      "// Send sends a message to the specified recipient and subject\n",
      "func (d *Dialer) Send(recipient string, subject string, data []byte) error {\n",
      "\tlog.Infof(\"Sending email to %s\", recipient)\n",
      "\tlog.Infof(\"Subject: %s\", subject)\n",
      "\tlog.Infof(\"Data: %s\", string(data))\n",
      "\tlog.Infof(\"Content-Type: application/x-www-form-urlencoded\")\n",
      "\treturn d.SendBytes(recipient, subject, data)\n",
      "}\n",
      "\n",
      "// Send sends a message to the specified recipient and subject\n",
      "func (d *Dialer) SendBytes(recipient string, subject string, data []byte) error {\n",
      "\tlog.Infof(\"Sending email to %s\", recipient)\n",
      "\tlog.Infof(\"Subject: %s\", subject)\n",
      "\tlog.Infof(\"Data: %s\", string(data))\n",
      "\treturn d.Dialer.Dial()\n",
      "}\n",
      "\n",
      "// Close closes the connection\n",
      "func (d *Dialer) Close() error {\n",
      "\treturn d.Dialer.Close()\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean up memory\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# Use the device that the model was loaded on\n",
    "device = model.device\n",
    "print(f\"Model is on device: {device}\")\n",
    "\n",
    "# Test base model before fine-tuning\n",
    "question = dataset[10]['input']\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt_style.format(question)],\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ").to(device)\n",
    "\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory allocated: {torch.cuda.memory_allocated()/1024**2:.2f} MB\")\n",
    "        print(f\"GPU Memory reserved: {torch.cuda.memory_reserved()/1024**2:.2f} MB\")\n",
    "        print(f\"Max memory allocated: {torch.cuda.max_memory_allocated()/1024**2:.2f} MB\")\n",
    "\n",
    "# Call before model.generate()\n",
    "print_gpu_memory()\n",
    "\n",
    "print(\"Testing base model...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_new_tokens=1200,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(\"Base model response:\")\n",
    "print(response[0].split(\"### Response:\")[1] if \"### Response:\" in response[0] else response[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a110c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,971,520 || all params: 377,683,968 || trainable%: 5.5527\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# LoRA config adapted for CodeGen-350M\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,                           # Scaling factor for LoRA\n",
    "    lora_dropout=0.05,                       # Add slight dropout for regularization\n",
    "    r=64,                                    # Rank of the LoRA update matrices\n",
    "    bias=\"none\",                             # No bias reparameterization\n",
    "    task_type=\"CAUSAL_LM\",                   # Task type: Causal Language Modeling\n",
    "    target_modules=[\n",
    "        \"qkv_proj\",                          # CodeGen uses qkv_proj instead of separate q,k,v\n",
    "        \"out_proj\",                          # Output projection\n",
    "        \"fc_in\",                             # Feed-forward input\n",
    "        \"fc_out\",                            # Feed-forward output\n",
    "    ],  # Target modules for LoRA (adapted for CodeGen architecture)\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7dbbf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding EOS to train dataset:  21%|██▏       | 999/4656 [00:00<00:00, 4765.14 examples/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 122] Disk quota exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/datasets/arrow_dataset.py:3517\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3516\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3517\u001b[39m         \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3518\u001b[39m num_examples_progress_update += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/datasets/arrow_writer.py:552\u001b[39m, in \u001b[36mArrowWriter.write\u001b[39m\u001b[34m(self, example, key, writer_batch_size)\u001b[39m\n\u001b[32m    550\u001b[39m     \u001b[38;5;28mself\u001b[39m.hkey_record = []\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_examples_on_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/datasets/arrow_writer.py:510\u001b[39m, in \u001b[36mArrowWriter.write_examples_on_file\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    506\u001b[39m         batch_examples[col] = [\n\u001b[32m    507\u001b[39m             row[\u001b[32m0\u001b[39m][col].to_pylist()[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row[\u001b[32m0\u001b[39m][col], (pa.Array, pa.ChunkedArray)) \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[32m0\u001b[39m][col]\n\u001b[32m    508\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_examples\n\u001b[32m    509\u001b[39m         ]\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[38;5;28mself\u001b[39m.current_examples = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/datasets/arrow_writer.py:630\u001b[39m, in \u001b[36mArrowWriter.write_batch\u001b[39m\u001b[34m(self, batch_examples, writer_batch_size, try_original_type)\u001b[39m\n\u001b[32m    629\u001b[39m pa_table = pa.Table.from_arrays(arrays, schema=schema)\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/datasets/arrow_writer.py:648\u001b[39m, in \u001b[36mArrowWriter.write_table\u001b[39m\u001b[34m(self, pa_table, writer_batch_size)\u001b[39m\n\u001b[32m    647\u001b[39m \u001b[38;5;28mself\u001b[39m._num_examples += pa_table.num_rows\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpa_writer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/pyarrow/ipc.pxi:529\u001b[39m, in \u001b[36mpyarrow.lib._CRecordBatchWriter.write_table\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/pyarrow/error.pxi:89\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/fsspec/implementations/local.py:432\u001b[39m, in \u001b[36mLocalFileOpener.write\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 122] Disk quota exceeded",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m      5\u001b[39m training_arguments = TrainingArguments(\n\u001b[32m      6\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33mcodegen_output\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     per_device_train_batch_size=\u001b[32m2\u001b[39m,           \u001b[38;5;66;03m# Increased batch size for smaller model\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     remove_unused_columns=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Cell 11: Initialize Trainer\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Initialize the SFT Trainer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m trainer = \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_arguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# dataset_text_field=\"text\",               # Specify the text field\u001b[39;49;00m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# max_seq_length=2048,                     # Set max sequence length\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# tokenizer=tokenizer,\u001b[39;49;00m\n\u001b[32m     38\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:473\u001b[39m, in \u001b[36mSFTTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[39m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.completion_only_loss \u001b[38;5;129;01mand\u001b[39;00m formatting_func:\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    468\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA formatting function was provided while `completion_only_loss=True`, which is incompatible. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    469\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing a formatter converts the dataset to a language modeling type, conflicting with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    470\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcompletion-only loss. To resolve this, apply your formatting function before passing the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    471\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdataset, or disable `completion_only_loss` in `SFTConfig`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    472\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m train_dataset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpacking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    475\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    477\u001b[39m     packing = args.packing \u001b[38;5;28;01mif\u001b[39;00m args.eval_packing \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m args.eval_packing\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:706\u001b[39m, in \u001b[36mSFTTrainer._prepare_dataset\u001b[39m\u001b[34m(self, dataset, processing_class, args, packing, formatting_func, dataset_name)\u001b[39m\n\u001b[32m    703\u001b[39m             example[\u001b[33m\"\u001b[39m\u001b[33mcompletion\u001b[39m\u001b[33m\"\u001b[39m] = example[\u001b[33m\"\u001b[39m\u001b[33mcompletion\u001b[39m\u001b[33m\"\u001b[39m] + eos_token\n\u001b[32m    704\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m example\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m     dataset = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_eos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meos_token\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m        \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# renamed to \"text\"\u001b[39;49;00m\n\u001b[32m    710\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmap_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[38;5;66;03m# Tokenize the dataset\u001b[39;00m\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, Dataset):  \u001b[38;5;66;03m# `IterableDataset.map` does not support `desc`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/datasets/arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/datasets/arrow_dataset.py:3079\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3074\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3075\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3076\u001b[39m         total=pbar_total,\n\u001b[32m   3077\u001b[39m         desc=desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3078\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3079\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/datasets/arrow_dataset.py:3552\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[32m   3551\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3552\u001b[39m         \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tmp_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3554\u001b[39m         tmp_file.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/datasets/arrow_writer.py:657\u001b[39m, in \u001b[36mArrowWriter.finalize\u001b[39m\u001b[34m(self, close_stream)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# Re-initializing to empty list for next batch\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28mself\u001b[39m.hkey_record = []\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_examples_on_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;66;03m# If schema is known, infer features even if no examples were written\u001b[39;00m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.schema:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/datasets/arrow_writer.py:510\u001b[39m, in \u001b[36mArrowWriter.write_examples_on_file\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    505\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    506\u001b[39m         batch_examples[col] = [\n\u001b[32m    507\u001b[39m             row[\u001b[32m0\u001b[39m][col].to_pylist()[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row[\u001b[32m0\u001b[39m][col], (pa.Array, pa.ChunkedArray)) \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[32m0\u001b[39m][col]\n\u001b[32m    508\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_examples\n\u001b[32m    509\u001b[39m         ]\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[38;5;28mself\u001b[39m.current_examples = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/datasets/arrow_writer.py:630\u001b[39m, in \u001b[36mArrowWriter.write_batch\u001b[39m\u001b[34m(self, batch_examples, writer_batch_size, try_original_type)\u001b[39m\n\u001b[32m    628\u001b[39m schema = inferred_features.arrow_schema \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.schema\n\u001b[32m    629\u001b[39m pa_table = pa.Table.from_arrays(arrays, schema=schema)\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/datasets/arrow_writer.py:648\u001b[39m, in \u001b[36mArrowWriter.write_table\u001b[39m\u001b[34m(self, pa_table, writer_batch_size)\u001b[39m\n\u001b[32m    646\u001b[39m \u001b[38;5;28mself\u001b[39m._num_bytes += pa_table.nbytes\n\u001b[32m    647\u001b[39m \u001b[38;5;28mself\u001b[39m._num_examples += pa_table.num_rows\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpa_writer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/pyarrow/ipc.pxi:529\u001b[39m, in \u001b[36mpyarrow.lib._CRecordBatchWriter.write_table\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/pyarrow/error.pxi:89\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/fsspec/implementations/local.py:432\u001b[39m, in \u001b[36mLocalFileOpener.write\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 122] Disk quota exceeded"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training Arguments (adjusted for CodeGen-350M)\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"codegen_output\",\n",
    "    per_device_train_batch_size=2,           # Increased batch size for smaller model\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=3,                      # Reduced epochs for faster training\n",
    "    logging_steps=0.1,                       # More frequent logging\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=3e-4,                      # Slightly higher LR for smaller model\n",
    "    fp16=False,\n",
    "    bf16=True,                               # Use bf16 if available\n",
    "    group_by_length=True,\n",
    "    report_to=\"wandb\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    eval_strategy=\"no\",\n",
    "    max_grad_norm=1.0,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Cell 11: Initialize Trainer\n",
    "# Initialize the SFT Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    data_collator=data_collator,\n",
    "    # dataset_text_field=\"text\",               # Specify the text field\n",
    "    # max_seq_length=2048,                     # Set max sequence length\n",
    "    # tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "812ab264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda:0\n",
      "Base model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Clean up memory before training\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# Ensure the trainer uses the same device as the model\n",
    "device = model.device\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "# Check if the model is properly loaded on the expected device\n",
    "if hasattr(model, 'base_model'):\n",
    "    print(f\"Base model device: {model.base_model.device}\")\n",
    "elif hasattr(model, 'model'):\n",
    "    print(f\"Model's model device: {model.model.device}\")\n",
    "\n",
    "# Make sure all model parts are on the same device\n",
    "model = model.to(device)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ce1e2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m3905020\u001b[0m (\u001b[33m3905020-universitiet-leiden\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/local/s3905020/thesis-slm/trial_scripts/codegen/wandb/run-20250717_050920-te3rrgws</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/3905020-universitiet-leiden/huggingface/runs/te3rrgws' target=\"_blank\">codegen_output</a></strong> to <a href='https://wandb.ai/3905020-universitiet-leiden/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/3905020-universitiet-leiden/huggingface' target=\"_blank\">https://wandb.ai/3905020-universitiet-leiden/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/3905020-universitiet-leiden/huggingface/runs/te3rrgws' target=\"_blank\">https://wandb.ai/3905020-universitiet-leiden/huggingface/runs/te3rrgws</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:767\u001b[39m, in \u001b[36mBatchEncoding.convert_to_tensors\u001b[39m\u001b[34m(self, tensor_type, prepend_batch_axis)\u001b[39m\n\u001b[32m    766\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[32m--> \u001b[39m\u001b[32m767\u001b[39m     tensor = \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    769\u001b[39m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[32m    770\u001b[39m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[32m    771\u001b[39m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[32m    772\u001b[39m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[32m    773\u001b[39m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[32m    774\u001b[39m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:729\u001b[39m, in \u001b[36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[39m\u001b[34m(value, dtype)\u001b[39m\n\u001b[32m    728\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.from_numpy(np.array(value))\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: too many dimensions 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/transformers/trainer.py:2206\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2204\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2205\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2207\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2211\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/transformers/trainer.py:2502\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2500\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2501\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2502\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2503\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[32m   2504\u001b[39m     step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/transformers/trainer.py:5300\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5298\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5299\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5300\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5301\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5302\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/accelerate/data_loader.py:567\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m     current_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/transformers/data/data_collator.py:46\u001b[39m, in \u001b[36mDataCollatorMixin.__call__\u001b[39m\u001b[34m(self, features, return_tensors)\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tf_call(features)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_tensors == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtorch_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_tensors == \u001b[33m\"\u001b[39m\u001b[33mnp\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy_call(features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/transformers/data/data_collator.py:1014\u001b[39m, in \u001b[36mDataCollatorForLanguageModeling.torch_call\u001b[39m\u001b[34m(self, examples)\u001b[39m\n\u001b[32m   1011\u001b[39m     \u001b[38;5;28mself\u001b[39m.create_rng()\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples[\u001b[32m0\u001b[39m], Mapping):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     batch = \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1018\u001b[39m     batch = {\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: _torch_collate_batch(examples, \u001b[38;5;28mself\u001b[39m.tokenizer, pad_to_multiple_of=\u001b[38;5;28mself\u001b[39m.pad_to_multiple_of)\n\u001b[32m   1020\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/transformers/data/data_collator.py:67\u001b[39m, in \u001b[36mpad_without_fast_tokenizer_warning\u001b[39m\u001b[34m(tokenizer, *pad_args, **pad_kwargs)\u001b[39m\n\u001b[32m     64\u001b[39m tokenizer.deprecation_warnings[\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     padded = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[32m     70\u001b[39m     tokenizer.deprecation_warnings[\u001b[33m\"\u001b[39m\u001b[33mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[33m\"\u001b[39m] = warning_state\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3374\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.pad\u001b[39m\u001b[34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[39m\n\u001b[32m   3371\u001b[39m             batch_outputs[key] = []\n\u001b[32m   3372\u001b[39m         batch_outputs[key].append(value)\n\u001b[32m-> \u001b[39m\u001b[32m3374\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:240\u001b[39m, in \u001b[36mBatchEncoding.__init__\u001b[39m\u001b[34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[39m\n\u001b[32m    236\u001b[39m     n_sequences = encoding[\u001b[32m0\u001b[39m].n_sequences\n\u001b[32m    238\u001b[39m \u001b[38;5;28mself\u001b[39m._n_sequences = n_sequences\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/local/s3905020/slm-go/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:783\u001b[39m, in \u001b[36mBatchEncoding.convert_to_tensors\u001b[39m\u001b[34m(self, tensor_type, prepend_batch_axis)\u001b[39m\n\u001b[32m    778\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33moverflowing_tokens\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    779\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    780\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    781\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    782\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m783\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    784\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    785\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpadding=True\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtruncation=True\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    786\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    787\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m expected).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    788\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    790\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78334cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"codegen_final_model\")\n",
    "tokenizer.save_pretrained(\"codegen_final_model\")\n",
    "print(\"Model saved to codegen_final_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ab2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing fine-tuned model...\")\n",
    "\n",
    "# Test on a different sample\n",
    "question = dataset[100]['input']\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt_style.format(question)],\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=2048\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_new_tokens=1200,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        use_cache=True,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(\"Fine-tuned model response:\")\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799bbdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer_codegen(model_dir=\"Salesforce/codegen-350M-multi\", \n",
    "                                     checkpoint_path=\"./codegen_output/checkpoint-XXX\",\n",
    "                                     device_str=\"cuda:0\"):\n",
    "    \"\"\"Load fine-tuned CodeGen model for evaluation\"\"\"\n",
    "    print(f\"Loading model from {model_dir} and checkpoint from {checkpoint_path}\")\n",
    "    \n",
    "    # Configure quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    # Set device\n",
    "    if device_str.startswith(\"cuda\"):\n",
    "        device_id = int(device_str.split(\":\")[-1])\n",
    "        torch.cuda.set_device(device_id)\n",
    "        print(f\"Using CUDA device: {torch.cuda.get_device_name(device_id)}\")\n",
    "    \n",
    "    device = torch.device(device_str)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load base model with quantization\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": device.index if device.type == \"cuda\" else \"cpu\"},\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Load the PEFT adapter\n",
    "    from peft import PeftModel\n",
    "    model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "    model.config.use_cache = True\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6cfa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset_by_tokens_codegen(file_path, max_tokens=100000, output_file=None):\n",
    "    \"\"\"Filter dataset for CodeGen tokenizer\"\"\"\n",
    "    import json\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-multi\", use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    filtered_data = []\n",
    "    excluded_count = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    print(f\"Filtering dataset: {file_path}\")\n",
    "    print(f\"Maximum tokens allowed: {max_tokens}\")\n",
    "    \n",
    "    # Process each line in the JSONL file\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            total_count += 1\n",
    "            entry = json.loads(line)\n",
    "            \n",
    "            # Tokenize the input text\n",
    "            tokens = tokenizer(entry[\"input\"], return_length=True, truncation=False)\n",
    "            token_count = len(tokens[\"input_ids\"])\n",
    "            \n",
    "            # Include entry if it's below the token limit\n",
    "            if token_count < max_tokens:\n",
    "                filtered_data.append(entry)\n",
    "            else:\n",
    "                excluded_count += 1\n",
    "    \n",
    "    # Save filtered dataset if output file is specified\n",
    "    if output_file:\n",
    "        with open(output_file, 'w') as f:\n",
    "            for entry in filtered_data:\n",
    "                f.write(json.dumps(entry) + '\\n')\n",
    "        print(f\"Filtered dataset saved to: {output_file}\")\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Total entries: {total_count}\")\n",
    "    print(f\"Entries kept: {len(filtered_data)} ({len(filtered_data)/total_count*100:.2f}%)\")\n",
    "    print(f\"Entries excluded: {excluded_count} ({excluded_count/total_count*100:.2f}%)\")\n",
    "    \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f45e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_codegen_model(model, tokenizer, test_data, device, output_file=\"codegen_evaluation_results.jsonl\"):\n",
    "    \"\"\"Evaluate CodeGen model on test data\"\"\"\n",
    "    import json\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    print(f\"Evaluating CodeGen model on {len(test_data)} test samples\")\n",
    "    results = []\n",
    "    \n",
    "    for idx, item in enumerate(tqdm(test_data)):\n",
    "        code = item[\"input\"]\n",
    "        expected_output = item[\"output\"]\n",
    "        \n",
    "        prompt = inference_prompt_style.format(code)\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            [prompt], \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            max_length=2048\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_new_tokens=1200,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                use_cache=True,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        response = generated_text.split(\"### Response:\")[1].strip() if \"### Response:\" in generated_text else generated_text\n",
    "        \n",
    "        result = {\n",
    "            \"index\": idx,\n",
    "            \"input\": code,\n",
    "            \"expected_output\": expected_output,\n",
    "            \"model_output\": response,\n",
    "            \"full_response\": generated_text\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # Save results periodically\n",
    "        if idx % 10 == 0:\n",
    "            with open(output_file, 'w') as f:\n",
    "                for res in results:\n",
    "                    f.write(json.dumps(res) + '\\n')\n",
    "    \n",
    "    # Final save\n",
    "    with open(output_file, 'w') as f:\n",
    "        for res in results:\n",
    "            f.write(json.dumps(res) + '\\n')\n",
    "    \n",
    "    print(f\"Evaluation complete! Results saved to {output_file}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee6f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_memory():\n",
    "    \"\"\"Clean up GPU memory\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b184ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "evaluation:\n",
    "\n",
    "# 1. Filter test dataset\n",
    "test_file = \"/path/to/test.jsonl\"\n",
    "filtered_file = \"/path/to/test_filtered.jsonl\"\n",
    "filtered_data = filter_dataset_by_tokens_codegen(test_file, max_tokens=80000, output_file=filtered_file)\n",
    "\n",
    "# 2. Load fine-tuned model\n",
    "model, tokenizer, device = load_model_and_tokenizer_codegen(\n",
    "    checkpoint_path=\"./codegen_output/checkpoint-1000\"\n",
    ")\n",
    "\n",
    "# 3. Load test data\n",
    "import json\n",
    "test_data = []\n",
    "with open(filtered_file, 'r') as f:\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))\n",
    "\n",
    "# 4. Run evaluation\n",
    "results = evaluate_codegen_model(model, tokenizer, test_data, device)\n",
    "\n",
    "# 5. Calculate metrics (use the same metrics functions from the original notebook)\n",
    "# You can copy the calculate_metrics and visualization functions from the original notebook\n",
    "\"\"\"\n",
    "\n",
    "print(\"CodeGen-350M fine-tuning notebook ready!\")\n",
    "print(\"Adjust the dataset paths and run the cells sequentially.\")\n",
    "print(\"The model architecture differences from Qwen have been handled in the LoRA configuration.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
