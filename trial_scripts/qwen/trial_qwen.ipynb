{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f724d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "# !CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89e0890",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "743723f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/s3905020/slm-go/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Set CUDA device explicitly first\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model_dir = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=bnb_config,   \n",
    "    device_map={\"\": torch.cuda.current_device()},  # Explicitly set to current CUDA device\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True             \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66789bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training prompt\n",
    "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a cybersecurity expert with advanced knowledge in software security and common weakness enumerations(CWE). \n",
    "Look at the following code and classify it with the apropriate CWE's if it has any. \n",
    "\n",
    "### Code:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69b24e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for code, response in zip(inputs, outputs):\n",
    "        # Append the EOS token to the response if it's not already there\n",
    "        if not response.endswith(tokenizer.eos_token):\n",
    "            response += tokenizer.eos_token\n",
    "        text = train_prompt_style.format(code, response)\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee42c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_dir=\"/local/s3905020/code/dataset-creation\"\n",
    ")[\"train\"]\n",
    "# Formatting the dataset, create a string from the output field which is a list\n",
    "dataset = dataset.map(lambda x: {\"output\": [\" \".join(map(str, out)) if isinstance(out, list) else str(out) for out in x[\"output\"]]}, batched=True)\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")\n",
    "dataset[\"text\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad441ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"text\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87fd9e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c305cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a cybersecurity expert with advanced knowledge in software security and common weakness enumerations (CWE). \n",
    "Please look at the following code and classify it with the appropriate CWE's if it has any.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb36cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Use the device that the model was loaded on\n",
    "device = model.device\n",
    "print(f\"Model is on device: {device}\")\n",
    "\n",
    "# BASE MODEL TESTING\n",
    "question = dataset[10]['input']\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt_style.format(question) + tokenizer.eos_token],\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(response[0].split(\"### Response:\")[1])\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92e3008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61588a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,                          \n",
    "    lora_dropout=0.05,                      \n",
    "    r=64,                                    \n",
    "    bias=\"none\",                             \n",
    "    task_type=\"CAUSAL_LM\",                   \n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # Target modules for LoRA\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26bb476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "# Training Arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=4,\n",
    "    logging_steps=0.2,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24f040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Ensure the trainer uses the same device as the model\n",
    "device = model.device\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "# Check if the model is properly loaded on the expected device\n",
    "if hasattr(model, 'base_model'):\n",
    "    print(f\"Base model device: {model.base_model.device}\")\n",
    "elif hasattr(model, 'model'):\n",
    "    print(f\"Model's model device: {model.model.device}\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b994d7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = dataset[100]['input']\n",
    "inputs = tokenizer(\n",
    "    [inference_prompt_style.format(question) + tokenizer.eos_token],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9147ac5",
   "metadata": {},
   "source": [
    "## QWen 0.6B Evaluation\n",
    "Below are the cells to be run to evaluate the model on a test batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6fa900",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import json\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5758714",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model_and_tokenizer(model_dir=\"Qwen/Qwen3-0.6B\", \n",
    "                             checkpoint_path=\"/local/s3905020/thesis-slm/trial_scripts/qwen/output/checkpoint-4656\",\n",
    "                             device_str=\"cuda:0\"):\n",
    "    print(f\"Loading model from {model_dir} and checkpoint from {checkpoint_path}\")\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    if device_str.startswith(\"cuda\"):\n",
    "        device_id = int(device_str.split(\":\")[-1])\n",
    "        torch.cuda.set_device(device_id)\n",
    "        print(f\"Using CUDA device: {torch.cuda.get_device_name(device_id)}\")\n",
    "    \n",
    "    device = torch.device(device_str)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "    \n",
    "    # Load base model with quantization\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": device.index if device.type == \"cuda\" else \"cpu\"},\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Load the PEFT adapter\n",
    "    model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "    model.config.use_cache = True\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67008371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(test_file, max_samples=None):\n",
    "    print(f\"Loading test data from {test_file}\")\n",
    "    test_data = []\n",
    "    with open(test_file, 'r') as f:\n",
    "        for line in f:\n",
    "            test_data.append(json.loads(line))\n",
    "            if max_samples and len(test_data) >= max_samples:\n",
    "                break\n",
    "    return test_data\n",
    "\n",
    "def generate_inference_prompt(code):\n",
    "    return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a cybersecurity expert with advanced knowledge in software security and common weakness enumerations (CWE). \n",
    "Please look at the following code and classify it with the appropriate CWE's if it has any.\n",
    "\n",
    "### Question:\n",
    "{code}\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef4272",
   "metadata": {},
   "source": [
    "# Inference Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff45b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from /local/s3905020/code/dataset-creation/test.jsonl\n",
      "Randomly selected test data index: 54\n",
      "Loading model from Qwen/Qwen3-0.6B and checkpoint from /local/s3905020/thesis-slm/trial_scripts/qwen/output/checkpoint-4656\n",
      "Using CUDA device: NVIDIA GeForce RTX 3090\n",
      "Expected output: ['CWE-269']\n",
      "Model response: user\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. \n",
      "Write a response that appropriately completes the request. \n",
      "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
      "\n",
      "### Instruction:\n",
      "You are a cybersecurity expert with advanced knowledge in software security and common weakness enumerations (CWE). \n",
      "Please look at the following code and classify it with the appropriate CWE's if it has any.\n",
      "\n",
      "### Question:\n",
      "// Copyright 2014-2019 Ulrich Kunitz. All rights reserved.\n",
      "// Use of this source code is governed by a BSD-style\n",
      "// license that can be found in the LICENSE file.\n",
      "\n",
      "package xz\n",
      "\n",
      "import (\n",
      "\t\"errors\"\n",
      "\t\"fmt\"\n",
      "\t\"hash\"\n",
      "\t\"io\"\n",
      "\n",
      "\t\"github.com/ulikunitz/xz/lzma\"\n",
      ")\n",
      "\n",
      "// WriterConfig describe the parameters for an xz writer.\n",
      "type WriterConfig struct {\n",
      "\tProperties *lzma.Properties\n",
      "\tDictCap    int\n",
      "\tBufSize    int\n",
      "\tBlockSize  int64\n",
      "\t// checksum method: CRC32, CRC64 or SHA256 (default: CRC64)\n",
      "\tCheckSum byte\n",
      "\t// Forces NoChecksum (default: false)\n",
      "\tNoCheckSum bool\n",
      "\t// match algorithm\n",
      "\tMatcher lzma.MatchAlgorithm\n",
      "}\n",
      "\n",
      "// fill replaces zero values with default values.\n",
      "func (c *WriterConfig) fill() {\n",
      "\tif c.Properties == nil {\n",
      "\t\tc.Properties = &lzma.Properties{LC: 3, LP: 0, PB: 2}\n",
      "\t}\n",
      "\tif c.DictCap == 0 {\n",
      "\t\tc.DictCap = 8 * 1024 * 1024\n",
      "\t}\n",
      "\tif c.BufSize == 0 {\n",
      "\t\tc.BufSize = 4096\n",
      "\t}\n",
      "\tif c.BlockSize == 0 {\n",
      "\t\tc.BlockSize = maxInt64\n",
      "\t}\n",
      "\tif c.CheckSum == 0 {\n",
      "\t\tc.CheckSum = CRC64\n",
      "\t}\n",
      "\tif c.NoCheckSum {\n",
      "\t\tc.CheckSum = None\n",
      "\t}\n",
      "}\n",
      "\n",
      "// Verify checks the configuration for errors. Zero values will be\n",
      "// replaced by default values.\n",
      "func (c *WriterConfig) Verify() error {\n",
      "\tif c == nil {\n",
      "\t\treturn errors.New(\"xz: writer configuration is nil\")\n",
      "\t}\n",
      "\tc.fill()\n",
      "\tlc := lzma.Writer2Config{\n",
      "\t\tProperties: c.Properties,\n",
      "\t\tDictCap:    c.DictCap,\n",
      "\t\tBufSize:    c.BufSize,\n",
      "\t\tMatcher:    c.Matcher,\n",
      "\t}\n",
      "\tif err := lc.Verify(); err != nil {\n",
      "\t\treturn err\n",
      "\t}\n",
      "\tif c.BlockSize <= 0 {\n",
      "\t\treturn errors.New(\"xz: block size out of range\")\n",
      "\t}\n",
      "\tif err := verifyFlags(c.CheckSum); err != nil {\n",
      "\t\treturn err\n",
      "\t}\n",
      "\treturn nil\n",
      "}\n",
      "\n",
      "// filters creates the filter list for the given parameters.\n",
      "func (c *WriterConfig) filters() []filter {\n",
      "\treturn []filter{&lzmaFilter{int64(c.DictCap)}}\n",
      "}\n",
      "\n",
      "// maxInt64 defines the maximum 64-bit signed integer.\n",
      "const maxInt64 = 1<<63 - 1\n",
      "\n",
      "// verifyFilters checks the filter list for the length and the right\n",
      "// sequence of filters.\n",
      "func verifyFilters(f []filter) error {\n",
      "\tif len(f) == 0 {\n",
      "\t\treturn errors.New(\"xz: no filters\")\n",
      "\t}\n",
      "\tif len(f) > 4 {\n",
      "\t\treturn errors.New(\"xz: more than four filters\")\n",
      "\t}\n",
      "\tfor _, g := range f[:len(f)-1] {\n",
      "\t\tif g.last() {\n",
      "\t\t\treturn errors.New(\"xz: last filter is not last\")\n",
      "\t\t}\n",
      "\t}\n",
      "\tif !f[len(f)-1].last() {\n",
      "\t\treturn errors.New(\"xz: wrong last filter\")\n",
      "\t}\n",
      "\treturn nil\n",
      "}\n",
      "\n",
      "// newFilterWriteCloser converts a filter list into a WriteCloser that\n",
      "// can be used by a blockWriter.\n",
      "func (c *WriterConfig) newFilterWriteCloser(w io.Writer, f []filter) (fw io.WriteCloser, err error) {\n",
      "\tif err = verifyFilters(f); err != nil {\n",
      "\t\treturn nil, err\n",
      "\t}\n",
      "\tfw = nopWriteCloser(w)\n",
      "\tfor i := len(f) - 1; i >= 0; i-- {\n",
      "\t\tfw, err = f[i].writeCloser(fw, c)\n",
      "\t\tif err != nil {\n",
      "\t\t\treturn nil, err\n",
      "\t\t}\n",
      "\t}\n",
      "\treturn fw, nil\n",
      "}\n",
      "\n",
      "// nopWCloser implements a WriteCloser with a Close method not doing\n",
      "// anything.\n",
      "type nopWCloser struct {\n",
      "\tio.Writer\n",
      "}\n",
      "\n",
      "// Close returns nil and doesn't do anything else.\n",
      "func (c nopWCloser) Close() error {\n",
      "\treturn nil\n",
      "}\n",
      "\n",
      "// nopWriteCloser converts the Writer into a WriteCloser with a Close\n",
      "// function that does nothing beside returning nil.\n",
      "func nopWriteCloser(w io.Writer) io.WriteCloser {\n",
      "\treturn nopWCloser{w}\n",
      "}\n",
      "\n",
      "// Writer compresses data written to it. It is an io.WriteCloser.\n",
      "type Writer struct {\n",
      "\tWriterConfig\n",
      "\n",
      "\txz      io.Writer\n",
      "\tbw      *blockWriter\n",
      "\tnewHash func() hash.Hash\n",
      "\th       header\n",
      "\tindex   []record\n",
      "\tclosed  bool\n",
      "}\n",
      "\n",
      "// newBlockWriter creates a new block writer writes the header out.\n",
      "func (w *Writer) newBlockWriter() error {\n",
      "\tvar err error\n",
      "\tw.bw, err = w.WriterConfig.newBlockWriter(w.xz, w.newHash())\n",
      "\tif err != nil {\n",
      "\t\treturn err\n",
      "\t}\n",
      "\tif err = w.bw.writeHeader(w.xz); err != nil {\n",
      "\t\treturn err\n",
      "\t}\n",
      "\treturn nil\n",
      "}\n",
      "\n",
      "// closeBlockWriter closes a block writer and records the sizes in the\n",
      "// index.\n",
      "func (w *Writer) closeBlockWriter() error {\n",
      "\tvar err error\n",
      "\tif err = w.bw.Close(); err != nil {\n",
      "\t\treturn err\n",
      "\t}\n",
      "\tw.index = append(w.index, w.bw.record())\n",
      "\treturn nil\n",
      "}\n",
      "\n",
      "// NewWriter creates a new xz writer using default parameters.\n",
      "func NewWriter(xz io.Writer) (w *Writer, err error) {\n",
      "\treturn WriterConfig{}.NewWriter(xz)\n",
      "}\n",
      "\n",
      "// NewWriter creates a new Writer using the given configuration parameters.\n",
      "func (c WriterConfig) NewWriter(xz io.Writer) (w *Writer, err error) {\n",
      "\tif err = c.Verify(); err != nil {\n",
      "\t\treturn nil, err\n",
      "\t}\n",
      "\tw = &Writer{\n",
      "\t\tWriterConfig: c,\n",
      "\t\txz:           xz,\n",
      "\t\th:            header{c.CheckSum},\n",
      "\t\tindex:        make([]record, 0, 4),\n",
      "\t}\n",
      "\tif w.newHash, err = newHashFunc(c.CheckSum); err != nil {\n",
      "\t\treturn nil, err\n",
      "\t}\n",
      "\tdata, err := w.h.MarshalBinary()\n",
      "\tif err != nil {\n",
      "\t\treturn nil, fmt.Errorf(\"w.h.MarshalBinary(): error %w\", err)\n",
      "\t}\n",
      "\tif _, err = xz.Write(data); err != nil {\n",
      "\t\treturn nil, err\n",
      "\t}\n",
      "\tif err = w.newBlockWriter(); err != nil {\n",
      "\t\treturn nil, err\n",
      "\t}\n",
      "\treturn w, nil\n",
      "\n",
      "}\n",
      "\n",
      "// Write compresses the uncompressed data provided.\n",
      "func (w *Writer) Write(p []byte) (n int, err error) {\n",
      "\tif w.closed {\n",
      "\t\treturn 0, errClosed\n",
      "\t}\n",
      "\tfor {\n",
      "\t\tk, err := w.bw.Write(p[n:])\n",
      "\t\tn += k\n",
      "\t\tif err != errNoSpace {\n",
      "\t\t\treturn n, err\n",
      "\t\t}\n",
      "\t\tif err = w.closeBlockWriter(); err != nil {\n",
      "\t\t\treturn n, err\n",
      "\t\t}\n",
      "\t\tif err = w.newBlockWriter(); err != nil {\n",
      "\t\t\treturn n, err\n",
      "\t\t}\n",
      "\t}\n",
      "}\n",
      "\n",
      "// Close closes the writer and adds the footer to the Writer. Close\n",
      "// doesn't close the underlying writer.\n",
      "func (w *Writer) Close() error {\n",
      "\tif w.closed {\n",
      "\t\treturn errClosed\n",
      "\t}\n",
      "\tw.closed = true\n",
      "\tvar err error\n",
      "\tif err = w.closeBlockWriter(); err != nil {\n",
      "\t\treturn err\n",
      "\t}\n",
      "\n",
      "\tf := footer{flags: w.h.flags}\n",
      "\tif f.indexSize, err = writeIndex(w.xz, w.index); err != nil {\n",
      "\t\treturn err\n",
      "\t}\n",
      "\tdata, err := f.MarshalBinary()\n",
      "\tif err != nil {\n",
      "\t\treturn err\n",
      "\t}\n",
      "\tif _, err = w.xz.Write(data); err != nil {\n",
      "\t\treturn err\n",
      "\t}\n",
      "\treturn nil\n",
      "}\n",
      "\n",
      "// countingWriter is a writer that counts all data written to it.\n",
      "type countingWriter struct {\n",
      "\tw io.Writer\n",
      "\tn int64\n",
      "}\n",
      "\n",
      "// Write writes data to the countingWriter.\n",
      "func (cw *countingWriter) Write(p []byte) (n int, err error) {\n",
      "\tn, err = cw.w.Write(p)\n",
      "\tcw.n += int64(n)\n",
      "\tif err == nil && cw.n < 0 {\n",
      "\t\treturn n, errors.New(\"xz: counter overflow\")\n",
      "\t}\n",
      "\treturn\n",
      "}\n",
      "\n",
      "// blockWriter is writes a single block.\n",
      "type blockWriter struct {\n",
      "\tcxz countingWriter\n",
      "\t// mw combines io.WriteCloser w and the hash.\n",
      "\tmw        io.Writer\n",
      "\tw         io.WriteCloser\n",
      "\tn         int64\n",
      "\tblockSize int64\n",
      "\tclosed    bool\n",
      "\theaderLen int\n",
      "\n",
      "\tfilters []filter\n",
      "\thash    hash.Hash\n",
      "}\n",
      "\n",
      "// newBlockWriter creates a new block writer.\n",
      "func (c *WriterConfig) newBlockWriter(xz io.Writer, hash hash.Hash) (bw *blockWriter, err error) {\n",
      "\tbw = &blockWriter{\n",
      "\t\tcxz:       countingWriter{w: xz},\n",
      "\t\tblockSize: c.BlockSize,\n",
      "\t\tfilters:   c.filters(),\n",
      "\t\thash:      hash,\n",
      "\t}\n",
      "\tbw.w, err = c.newFilterWriteCloser(&bw.cxz, bw.filters)\n",
      "\tif err != nil {\n",
      "\t\treturn nil, err\n",
      "\t}\n",
      "\tif bw.hash.Size() != 0 {\n",
      "\t\tbw.mw = io.MultiWriter(bw.w, bw.hash)\n",
      "\t} else {\n",
      "\t\tbw.mw = bw.w\n",
      "\t}\n",
      "\treturn bw, nil\n",
      "}\n",
      "\n",
      "// writeHeader writes the header. If the function is called after Close\n",
      "// the commpressedSize and uncompressedSize fields will be filled.\n",
      "func (bw *blockWriter) writeHeader(w io.Writer) error {\n",
      "\th := blockHeader{\n",
      "\t\tcompressedSize:   -1,\n",
      "\t\tuncompressedSize: -1,\n",
      "\t\tfilters:          bw.filters,\n",
      "\t}\n",
      "\tif bw.closed {\n",
      "\t\th.compressedSize = bw.compressedSize()\n",
      "\t\th.uncompressedSize = bw.uncompressedSize()\n",
      "\t}\n",
      "\tdata, err := h.MarshalBinary()\n",
      "\tif err != nil {\n",
      "\t\treturn err\n",
      "\t}\n",
      "\tif _, err = w.Write(data); err != nil {\n",
      "\t\treturn err\n",
      "\t}\n",
      "\tbw.headerLen = len(data)\n",
      "\treturn nil\n",
      "}\n",
      "\n",
      "// compressed size returns the amount of data written to the underlying\n",
      "// stream.\n",
      "func (bw *blockWriter) compressedSize() int64 {\n",
      "\treturn bw.cxz.n\n",
      "}\n",
      "\n",
      "// uncompressedSize returns the number of data written to the\n",
      "// blockWriter\n",
      "func (bw *blockWriter) uncompressedSize() int64 {\n",
      "\treturn bw.n\n",
      "}\n",
      "\n",
      "// unpaddedSize returns the sum of the header length, the uncompressed\n",
      "// size of the block and the hash size.\n",
      "func (bw *blockWriter) unpaddedSize() int64 {\n",
      "\tif bw.headerLen <= 0 {\n",
      "\t\tpanic(\"xz: block header not written\")\n",
      "\t}\n",
      "\tn := int64(bw.headerLen)\n",
      "\tn += bw.compressedSize()\n",
      "\tn += int64(bw.hash.Size())\n",
      "\treturn n\n",
      "}\n",
      "\n",
      "// record returns the record for the current stream. Call Close before\n",
      "// calling this method.\n",
      "func (bw *blockWriter) record() record {\n",
      "\treturn record{bw.unpaddedSize(), bw.uncompressedSize()}\n",
      "}\n",
      "\n",
      "var errClosed = errors.New(\"xz: writer already closed\")\n",
      "\n",
      "var errNoSpace = errors.New(\"xz: no space\")\n",
      "\n",
      "// Write writes uncompressed data to the block writer.\n",
      "func (bw *blockWriter) Write(p []byte) (n int, err error) {\n",
      "\tif bw.closed {\n",
      "\t\treturn 0, errClosed\n",
      "\t}\n",
      "\n",
      "\tt := bw.blockSize - bw.n\n",
      "\tif int64(len(p)) > t {\n",
      "\t\terr = errNoSpace\n",
      "\t\tp = p[:t]\n",
      "\t}\n",
      "\n",
      "\tvar werr error\n",
      "\tn, werr = bw.mw.Write(p)\n",
      "\tbw.n += int64(n)\n",
      "\tif werr != nil {\n",
      "\t\treturn n, werr\n",
      "\t}\n",
      "\treturn n, err\n",
      "}\n",
      "\n",
      "// Close closes the writer.\n",
      "func (bw *blockWriter) Close() error {\n",
      "\tif bw.closed {\n",
      "\t\treturn errClosed\n",
      "\t}\n",
      "\tbw.closed = true\n",
      "\tif err := bw.w.Close(); err != nil {\n",
      "\t\treturn err\n",
      "\t}\n",
      "\ts := bw.hash.Size()\n",
      "\tk := padLen(bw.cxz.n)\n",
      "\tp := make([]byte, k+s)\n",
      "\tbw.hash.Sum(p[k:k])\n",
      "\tif _, err := bw.cxz.w.Write(p); err != nil {\n",
      "\t\treturn err\n",
      "\t}\n",
      "\treturn nil\n",
      "}\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "assistant\n",
      "<think>\n",
      "Okay, let's try to figure out how to classify the provided code with the apropriate CWE's. \n",
      "\n",
      "First, I'll look at each part of the code and see if it does anything that might be missing. \n",
      "\n",
      "The code is a Go package that defines a Writer struct with various parameters. It includes a newBlockWriter function that creates a block writer based on the WriterConfig struct. \n",
      "\n",
      "Looking at the Writer struct, it has a newBlockWriter method which creates a new block writer with the given parameters. It also has a closeBlockWriter method which closes the block writer and records the sizes in the index. \n",
      "\n",
      "Then there's a verifyFilters function which checks the filters in the WriterConfig. If the filters are not valid, an error is returned. \n",
      "\n",
      "The filters are passed to the WriterConfig's newBlockWriter method. \n",
      "\n",
      "Finally, there's a newFilterWriteCloser function which converts the WriterConfig into a WriteCloser that can be used by a blockWriter. \n",
      "\n",
      "I don't see any CWE's in this code. It looks like it's a utility to create the Writer struct with the parameters specified. There might be some missing parts in the Writer struct, like a Close method, but I don't see any CWE's that apply here.\n",
      "</think>\n",
      "\n",
      "CWE-269\n",
      "assistant\n",
      "<think>\n",
      "Okay, let's try to figure out how to classify the provided code with the apropriate CWE's. \n",
      "\n",
      "First, I'll look at each part of the code and see if it does anything that might be missing. \n",
      "\n",
      "The code is a Go package that defines a Writer struct with various parameters. It includes a newBlockWriter function that creates a block writer based on the WriterConfig struct. \n",
      "\n",
      "Looking at the Writer struct, it has a newBlockWriter method which creates a new block writer with the given parameters. It also has a closeBlockWriter method which closes the block writer and records the sizes in the index. \n",
      "\n",
      "Then there's a verifyFilters function which checks the filters in the WriterConfig. If the filters are not valid, an error is returned. \n",
      "\n",
      "The filters are passed to the WriterConfig's newBlockWriter method. \n",
      "\n",
      "Finally, there's a newFilterWriteCloser function which converts the WriterConfig into a WriteCloser that can be used by a blockWriter. \n",
      "\n",
      "I don't see any CWE's in this code. It looks like it's a utility to create the Writer struct with the parameters specified. There might be some missing parts in the Writer struct, like a Close method, but I don't see any CWE's that apply here.\n",
      "</think>\n",
      "\n",
      "CWE-269\n"
     ]
    }
   ],
   "source": [
    "def inference_model(model, tokenizer, device, code):\n",
    "    prompt = generate_inference_prompt(code)\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=True,\n",
    "    )\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_new_tokens=32768,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True, \n",
    "    )\n",
    "    \n",
    "    # Get a random number between 0 and the length of test_data\n",
    "    print(f\"Expected output: {test_data[random_index]['output']}\")\n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    print(f\"Model response: {response[0]}\")\n",
    "    return response[0].split(\"### Response:\")[1].strip()\n",
    "\n",
    "test_data = load_test_data(\"/local/s3905020/code/dataset-creation/test.jsonl\")\n",
    "import random\n",
    "random_index = random.randint(0, len(test_data) - 1)\n",
    "print(f\"Randomly selected test data index: {random_index}\")\n",
    "code = test_data[random_index]['input']\n",
    "model, tokenizer, device = load_model_and_tokenizer()\n",
    "print(inference_model(model, tokenizer, device, code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8bf1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, test_data, device, output_file=\"evaluation_results.jsonl\"):\n",
    "    print(f\"Evaluating model on {len(test_data)} test samples\")\n",
    "    results = []\n",
    "    \n",
    "    for idx, item in enumerate(tqdm(test_data)):\n",
    "        code = item[\"input\"]\n",
    "        expected_output = item[\"output\"]\n",
    "        \n",
    "        prompt = generate_inference_prompt(code)\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=True, \n",
    "         ) \n",
    "        \n",
    "        inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_new_tokens=32000,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        response = generated_text.split(\"### Response:\")[1].strip() if \"### Response:\" in generated_text else generated_text\n",
    "        \n",
    "        result = {\n",
    "            \"index\": idx,\n",
    "            \"input\": code,\n",
    "            \"expected_output\": expected_output,\n",
    "            \"model_output\": response,\n",
    "            \"full_response\": generated_text\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # Save results after each prediction to avoid losing data if the notebook crashes\n",
    "        with open(output_file, 'w') as f:\n",
    "            for res in results:\n",
    "                f.write(json.dumps(res) + '\\n')\n",
    "    \n",
    "    print(f\"Evaluation complete! Results saved to {output_file}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84b4073",
   "metadata": {},
   "source": [
    "## Filter dataset\n",
    "Filter the dataset to have less than 120k tokens in the \"input\" section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f55c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter dataset by token count\n",
    "def filter_dataset_by_tokens(file_path, max_tokens=120000, output_file=None):\n",
    "    \"\"\"\n",
    "    Filter a JSONL dataset to include only entries with fewer than max_tokens tokens.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the input JSONL file\n",
    "        max_tokens (int): Maximum number of tokens allowed\n",
    "        output_file (str, optional): Path to save filtered dataset. If None, returns filtered data without saving.\n",
    "        \n",
    "    Returns:\n",
    "        list: Filtered dataset entries\n",
    "    \"\"\"\n",
    "    from transformers import AutoTokenizer\n",
    "    import json\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\", use_fast=True)\n",
    "    \n",
    "    filtered_data = []\n",
    "    excluded_count = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    print(f\"Filtering dataset: {file_path}\")\n",
    "    print(f\"Maximum tokens allowed: {max_tokens}\")\n",
    "    \n",
    "    # Process each line in the JSONL file\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            total_count += 1\n",
    "            entry = json.loads(line)\n",
    "            \n",
    "            tokens = tokenizer(entry[\"input\"], return_length=True)\n",
    "            token_count = tokens[\"length\"][0]\n",
    "\n",
    "            if token_count < max_tokens:\n",
    "                filtered_data.append(entry)\n",
    "            else:\n",
    "                excluded_count += 1\n",
    "\n",
    "    if output_file:\n",
    "        with open(output_file, 'w') as f:\n",
    "            for entry in filtered_data:\n",
    "                f.write(json.dumps(entry) + '\\n')\n",
    "        print(f\"Filtered dataset saved to: {output_file}\")\n",
    "    \n",
    "    print(f\"Total entries: {total_count}\")\n",
    "    print(f\"Entries kept: {len(filtered_data)} ({len(filtered_data)/total_count*100:.2f}%)\")\n",
    "    print(f\"Entries excluded: {excluded_count} ({excluded_count/total_count*100:.2f}%)\")\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "test_file = \"/local/s3905020/code/dataset-creation/test.jsonl\"\n",
    "filtered_file = \"/local/s3905020/code/dataset-creation/test_filtered.jsonl\"\n",
    "\n",
    "# Filter the dataset\n",
    "filtered_data = filter_dataset_by_tokens(\n",
    "    file_path=test_file,\n",
    "    max_tokens=80000,\n",
    "    output_file=filtered_file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb71b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_file = \"/local/s3905020/code/dataset-creation/test_filtered.jsonl\"\n",
    "output_file = \"evaluation_results.jsonl\"\n",
    "max_samples = None  # Set to a number for a smaller test set\n",
    "\n",
    "# Clean up GPU memory\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model, tokenizer, device = load_model_and_tokenizer()\n",
    "test_data = load_test_data(test_file, max_samples)\n",
    "results = evaluate_model(model, tokenizer, test_data, device, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209feb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_cwe_numbers(text):\n",
    "    pattern = r'CWE[-\\s]*(\\d+)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return [int(cwe) for cwe in matches]\n",
    "\n",
    "def calculate_metrics(results):\n",
    "    metrics = {}\n",
    "    \n",
    "    total = len(results)\n",
    "    exact_matches = 0\n",
    "    partial_matches = 0\n",
    "    no_matches = 0\n",
    "    false_positives = 0\n",
    "    \n",
    "    cwe_actual_counts = {}\n",
    "    cwe_predicted_counts = {}\n",
    "    cwe_correct_counts = {}\n",
    "\n",
    "    all_actual_cwes = []\n",
    "    all_predicted_cwes = []\n",
    "    \n",
    "    for result in results:\n",
    "        expected_output = result[\"expected_output\"]\n",
    "        model_output = result[\"model_output\"]\n",
    "        \n",
    "        if isinstance(expected_output, list):\n",
    "            actual_cwes = [int(cwe) if isinstance(cwe, (int, str)) and str(cwe).isdigit() \n",
    "                          else extract_cwe_numbers(str(cwe)) for cwe in expected_output]\n",
    "            actual_cwes = [item for sublist in actual_cwes for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "        else:\n",
    "            actual_cwes = extract_cwe_numbers(str(expected_output))\n",
    "        \n",
    "        predicted_cwes = extract_cwe_numbers(model_output)\n",
    "        all_actual_cwes.append(set(actual_cwes))\n",
    "        all_predicted_cwes.append(set(predicted_cwes))\n",
    "        \n",
    "        for cwe in actual_cwes:\n",
    "            cwe_actual_counts[cwe] = cwe_actual_counts.get(cwe, 0) + 1\n",
    "        \n",
    "        for cwe in predicted_cwes:\n",
    "            cwe_predicted_counts[cwe] = cwe_predicted_counts.get(cwe, 0) + 1\n",
    "        \n",
    "        correct_cwes = set(actual_cwes).intersection(set(predicted_cwes))\n",
    "        for cwe in correct_cwes:\n",
    "            cwe_correct_counts[cwe] = cwe_correct_counts.get(cwe, 0) + 1\n",
    "        \n",
    "        if set(actual_cwes) == set(predicted_cwes):\n",
    "            exact_matches += 1\n",
    "        elif correct_cwes:\n",
    "            partial_matches += 1\n",
    "        else:\n",
    "            no_matches += 1\n",
    "    \n",
    "    metrics[\"total_samples\"] = total\n",
    "    metrics[\"exact_match_rate\"] = exact_matches / total if total > 0 else 0\n",
    "    metrics[\"partial_match_rate\"] = partial_matches / total if total > 0 else 0\n",
    "    metrics[\"no_match_rate\"] = no_matches / total if total > 0 else 0\n",
    "    \n",
    "    cwe_metrics = {}\n",
    "    all_cwes = sorted(set(list(cwe_actual_counts.keys()) + list(cwe_predicted_counts.keys())))\n",
    "    \n",
    "    for cwe in all_cwes:\n",
    "        true_positives = cwe_correct_counts.get(cwe, 0)\n",
    "        false_positives = cwe_predicted_counts.get(cwe, 0) - true_positives\n",
    "        false_negatives = cwe_actual_counts.get(cwe, 0) - true_positives\n",
    "        \n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        cwe_metrics[str(cwe)] = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"support\": cwe_actual_counts.get(cwe, 0)\n",
    "        }\n",
    "    \n",
    "    precisions = [m[\"precision\"] for m in cwe_metrics.values()]\n",
    "    recalls = [m[\"recall\"] for m in cwe_metrics.values()]\n",
    "    f1s = [m[\"f1\"] for m in cwe_metrics.values()]\n",
    "    supports = [m[\"support\"] for m in cwe_metrics.values()]\n",
    "    \n",
    "    metrics[\"macro_avg\"] = {\n",
    "        \"precision\": np.mean(precisions),\n",
    "        \"recall\": np.mean(recalls),\n",
    "        \"f1\": np.mean(f1s)\n",
    "    }\n",
    "    \n",
    "    total_support = sum(supports)\n",
    "    metrics[\"weighted_avg\"] = {\n",
    "        \"precision\": np.sum(np.multiply(precisions, supports)) / total_support if total_support > 0 else 0,\n",
    "        \"recall\": np.sum(np.multiply(recalls, supports)) / total_support if total_support > 0 else 0,\n",
    "        \"f1\": np.sum(np.multiply(f1s, supports)) / total_support if total_support > 0 else 0\n",
    "    }\n",
    "\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    for actual, predicted in zip(all_actual_cwes, all_predicted_cwes):\n",
    "        true_positives += len(actual.intersection(predicted))\n",
    "        false_positives += len(predicted - actual)\n",
    "        false_negatives += len(actual - predicted)\n",
    "    \n",
    "    overall_precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    overall_recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
    "    \n",
    "    metrics[\"overall\"] = {\n",
    "        \"precision\": overall_precision,\n",
    "        \"recall\": overall_recall,\n",
    "        \"f1\": overall_f1\n",
    "    }\n",
    "    \n",
    "    metrics[\"per_cwe\"] = cwe_metrics\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9816863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_visualizations(metrics, results, plots_dir=\"evaluation_plots\"):\n",
    "    \"\"\"Create visualization plots for the metrics.\"\"\"\n",
    "    import os\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    overall_metrics = [\n",
    "        metrics[\"overall\"][\"precision\"], \n",
    "        metrics[\"overall\"][\"recall\"], \n",
    "        metrics[\"overall\"][\"f1\"],\n",
    "        metrics[\"exact_match_rate\"],\n",
    "        metrics[\"partial_match_rate\"]\n",
    "    ]\n",
    "    labels = [\"Precision\", \"Recall\", \"F1\", \"Exact Match Rate\", \"Partial Match Rate\"]\n",
    "    \n",
    "    plt.bar(labels, overall_metrics, color='skyblue')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Overall Performance Metrics')\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.savefig(f\"{plots_dir}/overall_metrics.png\")\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    cwe_metrics = metrics[\"per_cwe\"]\n",
    "    # Sort by support (frequency)\n",
    "    sorted_cwes = sorted(cwe_metrics.items(), \n",
    "                         key=lambda x: x[1][\"support\"], \n",
    "                         reverse=True)[:15]  # Top 15 most frequent CWEs\n",
    "    \n",
    "    cwe_names = [f\"CWE-{cwe}\" for cwe, _ in sorted_cwes]\n",
    "    f1_scores = [m[\"f1\"] for _, m in sorted_cwes]\n",
    "    support = [m[\"support\"] for _, m in sorted_cwes]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = plt.gca()\n",
    "    bars = plt.bar(cwe_names, f1_scores, color='lightgreen')\n",
    "    \n",
    "\n",
    "    for i, (bar, sup) in enumerate(zip(bars, support)):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "                 str(sup), ha='center', va='bottom', rotation=0)\n",
    "    \n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('F1 Scores for Top CWEs (with support count)')\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_dir}/per_cwe_f1.png\")\n",
    "    plt.show()  # For notebook display\n",
    "    \n",
    "    top_cwes = [int(cwe) for cwe, _ in sorted_cwes[:10]]  # Top 10\n",
    "    \n",
    "    # Create a matrix of actual vs predicted\n",
    "    confusion = np.zeros((len(top_cwes), 2))  # Column 0: correct, Column 1: incorrect\n",
    "    \n",
    "    for result in results:\n",
    "        # Extract CWEs\n",
    "        if isinstance(result[\"expected_output\"], list):\n",
    "            actual_cwes = []\n",
    "            for cwe in result[\"expected_output\"]:\n",
    "                if isinstance(cwe, (int, str)) and str(cwe).isdigit():\n",
    "                    actual_cwes.append(int(cwe))\n",
    "                else:\n",
    "                    actual_cwes.extend(extract_cwe_numbers(str(cwe)))\n",
    "        else:\n",
    "            actual_cwes = extract_cwe_numbers(str(result[\"expected_output\"]))\n",
    "        \n",
    "        predicted_cwes = extract_cwe_numbers(result[\"model_output\"])\n",
    "        \n",
    "        # For each top CWE that's in the actual set\n",
    "        for i, cwe in enumerate(top_cwes):\n",
    "            if cwe in actual_cwes:\n",
    "                if cwe in predicted_cwes:\n",
    "                    confusion[i, 0] += 1  # Correct prediction\n",
    "                else:\n",
    "                    confusion[i, 1] += 1  # Missed prediction\n",
    "    \n",
    "    row_sums = confusion.sum(axis=1, keepdims=True)\n",
    "    confusion_pct = np.zeros_like(confusion) if np.all(row_sums == 0) else confusion / row_sums * 100\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(confusion_pct, annot=True, fmt='.1f', \n",
    "                xticklabels=['Detected', 'Missed'],\n",
    "                yticklabels=[f'CWE-{cwe}' for cwe in top_cwes],\n",
    "                cmap='YlGnBu')\n",
    "    plt.title('Detection Rate for Top CWEs (%)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_dir}/cwe_detection_rate.png\")\n",
    "    plt.show() \n",
    "    \n",
    "    return plots_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ccf4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "results_file = \"evaluation_results.jsonl\"\n",
    "output_file = \"evaluation_metrics.json\"\n",
    "plots_dir = \"evaluation_plots\"\n",
    "\n",
    "#Load results from file\n",
    "results = []\n",
    "with open(results_file, 'r') as f:\n",
    "    for line in f:\n",
    "        results.append(json.loads(line))\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = calculate_metrics(results)\n",
    "\n",
    "# Save metrics to file\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"Metrics saved to {output_file}\")\n",
    "\n",
    "# Create visualizations\n",
    "plots_path = create_visualizations(metrics, results, plots_dir)\n",
    "print(f\"Visualization plots saved to {plots_path}\")\n",
    "\n",
    "# Print summary metrics\n",
    "print(\"\\n=== EVALUATION SUMMARY ===\")\n",
    "print(f\"Total samples: {metrics['total_samples']}\")\n",
    "print(f\"Exact match rate: {metrics['exact_match_rate']:.4f}\")\n",
    "print(f\"Partial match rate: {metrics['partial_match_rate']:.4f}\")\n",
    "print(f\"Overall precision: {metrics['overall']['precision']:.4f}\")\n",
    "print(f\"Overall recall: {metrics['overall']['recall']:.4f}\")\n",
    "print(f\"Overall F1 score: {metrics['overall']['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048dcb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def display_example(result, index=None):\n",
    "    \"\"\"Display a single example with ground truth and prediction\"\"\"\n",
    "    idx = index if index is not None else result[\"index\"]\n",
    "    print(f\"Example #{idx}\")\n",
    "    print(\"\\n=== CODE ===\")\n",
    "    print(result[\"input\"][:500] + \"...\" if len(result[\"input\"]) > 500 else result[\"input\"])\n",
    "    \n",
    "    print(\"\\n=== EXPECTED OUTPUT ===\")\n",
    "    if isinstance(result[\"expected_output\"], list):\n",
    "        print(\", \".join(str(cwe) for cwe in result[\"expected_output\"]))\n",
    "    else:\n",
    "        print(result[\"expected_output\"])\n",
    "    \n",
    "    print(\"\\n=== MODEL OUTPUT ===\")\n",
    "    print(result[\"model_output\"][:1000] + \"...\" if len(result[\"model_output\"]) > 1000 else result[\"model_output\"])\n",
    "\n",
    "    if isinstance(result[\"expected_output\"], list):\n",
    "        actual_cwes = []\n",
    "        for cwe in result[\"expected_output\"]:\n",
    "            if isinstance(cwe, (int, str)) and str(cwe).isdigit():\n",
    "                actual_cwes.append(int(cwe))\n",
    "            else:\n",
    "                actual_cwes.extend(extract_cwe_numbers(str(cwe)))\n",
    "    else:\n",
    "        actual_cwes = extract_cwe_numbers(str(result[\"expected_output\"]))\n",
    "    \n",
    "    predicted_cwes = extract_cwe_numbers(result[\"model_output\"])\n",
    "    \n",
    "    print(\"\\n=== CWE COMPARISON ===\")\n",
    "    print(f\"Expected CWEs: {actual_cwes}\")\n",
    "    print(f\"Predicted CWEs: {predicted_cwes}\")\n",
    "    print(f\"Correct predictions: {set(actual_cwes).intersection(set(predicted_cwes))}\")\n",
    "    print(f\"Missed: {set(actual_cwes) - set(predicted_cwes)}\")\n",
    "    print(f\"Extra: {set(predicted_cwes) - set(actual_cwes)}\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Display a few random examples\n",
    "num_examples = 3\n",
    "# Get three random results from the evaluation\n",
    "import random\n",
    "random.seed(42)  # For reproducibility\n",
    "random_indices = random.sample(range(len(results)), num_examples)\n",
    "for idx in random_indices:\n",
    "    display_example(results[idx], index=idx)\n",
    "# for i in range(len(results)):\n",
    "#     display_example(results[i])\n",
    "\n",
    "# Or display a specific example\n",
    "# display_example(results[10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
