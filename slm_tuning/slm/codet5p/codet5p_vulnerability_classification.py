
import os
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split
from transformers import AutoTokenizer, T5EncoderModel
import pandas as pd
import numpy as np
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report
)
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import json
from datetime import datetime

DEVICE = torch.device("cuda:1" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")

CUSTOM_CACHE_DIR = "/local/s3905020/temp/"

# Hugging Face Transformers cache
os.environ["TRANSFORMERS_CACHE"] = CUSTOM_CACHE_DIR
# Hugging Face Datasets cache
os.environ["HF_DATASETS_CACHE"] = CUSTOM_CACHE_DIR
# Tokenizer cache
os.environ["HF_HOME"] = CUSTOM_CACHE_DIR

class VulnerabilityDataset(Dataset):
    
    def __init__(self, dataframe, tokenizer, max_length=512):
        self.data = dataframe.reset_index(drop=True)
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        input_code = str(row['code'])
        label = int(row['is_vulnerable'])
        
        tokenized = self.tokenizer(
            input_code,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': tokenized['input_ids'].squeeze(0),
            'attention_mask': tokenized['attention_mask'].squeeze(0),
            'labels': torch.tensor(label, dtype=torch.long)
        }
    
    def __len__(self):
        return len(self.data)

class CodeT5Classifier(nn.Module):
    
    def __init__(self, model_name, dropout=0.1):
        super().__init__()
        self.encoder = T5EncoderModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(self.encoder.config.d_model, 2)
    
    def forward(self, input_ids, attention_mask):
        encoder_output = self.encoder(
            input_ids=input_ids, 
            attention_mask=attention_mask
        )
        cls_embedding = encoder_output.last_hidden_state[:, 0, :]
        cls_embedding = self.dropout(cls_embedding)
        logits = self.classifier(cls_embedding)
        return logits

def train_epoch(model, dataloader, criterion, optimizer, device, use_amp=False):
    model.train()
    total_loss = 0
    predictions, true_labels = [], []
    
    scaler = torch.cuda.amp.GradScaler() if use_amp else None
    
    progress_bar = tqdm(dataloader, desc="Training")
    for batch in progress_bar:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        
        optimizer.zero_grad()
        
        if use_amp:
            with torch.cuda.amp.autocast():
                logits = model(input_ids=input_ids, attention_mask=attention_mask)
                loss = criterion(logits, labels)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            logits = model(input_ids=input_ids, attention_mask=attention_mask)
            loss = criterion(logits, labels)
            loss.backward()
            optimizer.step()
        
        total_loss += loss.item()
        preds = torch.argmax(logits, dim=-1)
        predictions.extend(preds.cpu().numpy())
        true_labels.extend(labels.cpu().numpy())
        
        progress_bar.set_postfix({'loss': loss.item()})
    
    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(true_labels, predictions)
    
    return avg_loss, accuracy

def evaluate(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    predictions, true_labels, probabilities = [], [], []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            logits = model(input_ids=input_ids, attention_mask=attention_mask)
            loss = criterion(logits, labels)
            
            total_loss += loss.item()
            probs = torch.softmax(logits, dim=-1)
            preds = torch.argmax(logits, dim=-1)
            
            predictions.extend(preds.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())
            probabilities.extend(probs[:, 1].cpu().numpy())
    
    avg_loss = total_loss / len(dataloader)
    
    metrics = {
        'loss': avg_loss,
        'accuracy': accuracy_score(true_labels, predictions),
        'precision': precision_score(true_labels, predictions, zero_division=0),
        'recall': recall_score(true_labels, predictions, zero_division=0),
        'f1': f1_score(true_labels, predictions, zero_division=0),
        'roc_auc': roc_auc_score(true_labels, probabilities) if len(set(true_labels)) > 1 else 0.0
    }
    
    return metrics, predictions, true_labels, probabilities

def plot_training_history(history, save_path='training_history.png'):
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o')
    axes[0, 0].plot(history['val_loss'], label='Val Loss', marker='s')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_title('Training and Validation Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)
    
    axes[0, 1].plot(history['train_acc'], label='Train Acc', marker='o')
    axes[0, 1].plot(history['val_acc'], label='Val Acc', marker='s')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Accuracy')
    axes[0, 1].set_title('Training and Validation Accuracy')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    axes[1, 0].plot(history['val_f1'], label='Val F1', marker='s', color='green')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('F1 Score')
    axes[1, 0].set_title('Validation F1 Score')
    axes[1, 0].legend()
    axes[1, 0].grid(True)
    
    axes[1, 1].plot(history['val_precision'], label='Precision', marker='o')
    axes[1, 1].plot(history['val_recall'], label='Recall', marker='s')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Score')
    axes[1, 1].set_title('Validation Precision and Recall')
    axes[1, 1].legend()
    axes[1, 1].grid(True)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Training history plot saved to {save_path}")

def plot_confusion_matrix(y_true, y_pred, save_path='confusion_matrix.png'):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['Not Vulnerable', 'Vulnerable'],
                yticklabels=['Not Vulnerable', 'Vulnerable'])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Confusion matrix saved to {save_path}")

def plot_roc_curve(y_true, y_probs, save_path='roc_curve.png'):
    from sklearn.metrics import roc_curve, auc
    
    fpr, tpr, thresholds = roc_curve(y_true, y_probs)
    roc_auc = auc(fpr, tpr)
    
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, 
             label=f'ROC curve (AUC = {roc_auc:.3f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"ROC curve saved to {save_path}")

def main():
    CONFIG = {
        'model_name': 'Salesforce/codet5p-770m',
        'data_path': './ds.jsonl',
        'max_length': 512,
        'batch_size': 8,
        'learning_rate': 2e-5,
        'num_epochs': 5,
        'dropout': 0.1,
        'train_split': 0.8,
        'val_split': 0.1,
        'test_split': 0.1,
        'random_seed': 42,
        'use_mixed_precision': True,
        'gradient_checkpointing': True,
        'gradient_accumulation_steps': 2,
    }
    
    torch.manual_seed(CONFIG['random_seed'])
    np.random.seed(CONFIG['random_seed'])
    
    if CONFIG['data_path'].endswith('.jsonl') or CONFIG['data_path'].endswith('.json'):
        print("  Detected JSONL format")
        data = []
        with open(CONFIG['data_path'], 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line.strip()))
        df = pd.DataFrame(data)
    else:
        print("  Detected CSV format")
        df = pd.read_csv(CONFIG['data_path'])

    print(f"Dataset size before filtering: {len(df)} samples")

    tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])
    def token_count(code):
        return len(tokenizer(code, truncation=False)['input_ids'])

    df['token_count'] = df['code'].apply(token_count)
    filtered_df = df[df['token_count'] <= 512].reset_index(drop=True)

    print(f"Dataset size after filtering (<=512 tokens): {len(filtered_df)} samples")
    print(f"Class distribution after filtering:\n{filtered_df['is_vulnerable'].value_counts()}")

    from sklearn.model_selection import train_test_split

    train_df, temp_df = train_test_split(
        filtered_df,
        test_size=CONFIG['val_split'] + CONFIG['test_split'],
        stratify=filtered_df['is_vulnerable'],
        random_state=CONFIG['random_seed']
    )

    val_size_adjusted = CONFIG['val_split'] / (CONFIG['val_split'] + CONFIG['test_split'])

    val_df, test_df = train_test_split(
        temp_df,
        test_size=1 - val_size_adjusted,
        stratify=temp_df['is_vulnerable'],
        random_state=CONFIG['random_seed']
    )

    print(f"  Train size: {len(train_df)} samples")
    print(train_df['is_vulnerable'].value_counts())
    print(f"  Val size: {len(val_df)} samples")
    print(val_df['is_vulnerable'].value_counts())
    print(f"  Test size: {len(test_df)} samples")
    print(test_df['is_vulnerable'].value_counts())

    train_dataset = VulnerabilityDataset(train_df, tokenizer, CONFIG['max_length'])
    val_dataset = VulnerabilityDataset(val_df, tokenizer, CONFIG['max_length'])
    test_dataset = VulnerabilityDataset(test_df, tokenizer, CONFIG['max_length'])

    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=4, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=4, pin_memory=True)

    print(f"\nLoading model: {CONFIG['model_name']}")
    tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])
    model = CodeT5Classifier(CONFIG['model_name'], dropout=CONFIG['dropout'])
    
    if CONFIG['gradient_checkpointing']:
        model.encoder.gradient_checkpointing_enable()
        print("✓ Gradient checkpointing enabled")
    
    model = model.to(DEVICE)
    print(f"✓ Model loaded on {DEVICE}")
    
    param_count = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nModel Parameters:")
    print(f"  Total: {param_count:,}")
    print(f"  Trainable: {trainable_params:,}")
    print(f"  Model size (FP32): ~{param_count * 4 / (1024**2):.2f} MB")
    print(f"  Model size (FP16): ~{param_count * 2 / (1024**2):.2f} MB")
    
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'])
    history = {
        'train_loss': [], 'train_acc': [],
        'val_loss': [], 'val_acc': [], 'val_precision': [],
        'val_recall': [], 'val_f1': [], 'val_roc_auc': []
    }
    
    best_val_f1 = 0.0
    for epoch in range(CONFIG['num_epochs']):
        print(f"\nEpoch {epoch + 1}/{CONFIG['num_epochs']}")
        
        train_loss, train_acc = train_epoch(
            model, train_loader, criterion, optimizer, DEVICE,
            use_amp=CONFIG['use_mixed_precision']
        )
        
        val_metrics, _, _, _ = evaluate(model, val_loader, criterion, DEVICE)
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_metrics['loss'])
        history['val_acc'].append(val_metrics['accuracy'])
        history['val_precision'].append(val_metrics['precision'])
        history['val_recall'].append(val_metrics['recall'])
        history['val_f1'].append(val_metrics['f1'])
        history['val_roc_auc'].append(val_metrics['roc_auc'])
        
        print(f"\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}")
        print(f"Val Loss: {val_metrics['loss']:.4f} | Val Acc: {val_metrics['accuracy']:.4f}")
        print(f"Val Precision: {val_metrics['precision']:.4f} | Val Recall: {val_metrics['recall']:.4f}")
        print(f"Val F1: {val_metrics['f1']:.4f} | Val ROC-AUC: {val_metrics['roc_auc']:.4f}")
        
        if val_metrics['f1'] > best_val_f1:
            best_val_f1 = val_metrics['f1']
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_f1': best_val_f1,
                'config': CONFIG
            }, 'best_model.pt')

    plot_training_history(history)
    
    # print("\nLoading best model for testing")
    checkpoint = torch.load('best_model.pt', map_location=DEVICE)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    test_metrics, test_preds, test_labels, test_probs = evaluate(
        model, test_loader, criterion, DEVICE
    )

    print(f"\nTest Results:")
    print(f"  Accuracy:  {test_metrics['accuracy']:.4f}")
    print(f"  Precision: {test_metrics['precision']:.4f}")
    print(f"  Recall:    {test_metrics['recall']:.4f}")
    print(f"  F1 Score:  {test_metrics['f1']:.4f}")
    print(f"  ROC-AUC:   {test_metrics['roc_auc']:.4f}")

    print(f"\nDetailed Classification Report:")
    print(classification_report(test_labels, test_preds, 
                                target_names=['Not Vulnerable', 'Vulnerable']))
    
    plot_confusion_matrix(test_labels, test_preds)
    plot_roc_curve(test_labels, test_probs)
    
    results = {
        'config': CONFIG,
        'best_val_f1': best_val_f1,
        'test_metrics': test_metrics,
        'training_history': history,
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    with open('training_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\n✓ Results saved to training_results.json")
    
    if torch.cuda.is_available():
        print("GPU Memory Summary:")
        print(f"  Allocated: {torch.cuda.memory_allocated(DEVICE) / 1024**2:.2f} MB")
        print(f"  Reserved:  {torch.cuda.memory_reserved(DEVICE) / 1024**2:.2f} MB")
        print(f"  Max Allocated: {torch.cuda.max_memory_allocated(DEVICE) / 1024**2:.2f} MB")

if __name__ == "__main__":
    main()
